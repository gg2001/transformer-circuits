{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [What is a Transformer?](https://youtu.be/bOYE6E8JrtU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gge/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,\n",
      "          6067,     0]], device='mps:0')\n",
      "tensor([[15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,  6067,\n",
      "             0]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\")\n",
    ")\n",
    "print(\n",
    "    reference_gpt2.to_tokens(\n",
    "        \"Whether a word begins with a capital or space matters!\", prepend_bos=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed.W_E', 'pos_embed.W_pos', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.mlp.W_in', 'blocks.4.mlp.b_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_out', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.mlp.W_in', 'blocks.5.mlp.b_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.b_out', 'blocks.6.ln1.w', 'blocks.6.ln1.b', 'blocks.6.ln2.w', 'blocks.6.ln2.b', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_O', 'blocks.6.attn.W_K', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_V', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.mlp.W_in', 'blocks.6.mlp.b_in', 'blocks.6.mlp.W_out', 'blocks.6.mlp.b_out', 'blocks.7.ln1.w', 'blocks.7.ln1.b', 'blocks.7.ln2.w', 'blocks.7.ln2.b', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_O', 'blocks.7.attn.W_K', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_V', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.mlp.W_in', 'blocks.7.mlp.b_in', 'blocks.7.mlp.W_out', 'blocks.7.mlp.b_out', 'blocks.8.ln1.w', 'blocks.8.ln1.b', 'blocks.8.ln2.w', 'blocks.8.ln2.b', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_O', 'blocks.8.attn.W_K', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_V', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.mlp.W_in', 'blocks.8.mlp.b_in', 'blocks.8.mlp.W_out', 'blocks.8.mlp.b_out', 'blocks.9.ln1.w', 'blocks.9.ln1.b', 'blocks.9.ln2.w', 'blocks.9.ln2.b', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_O', 'blocks.9.attn.W_K', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_V', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.mlp.W_in', 'blocks.9.mlp.b_in', 'blocks.9.mlp.W_out', 'blocks.9.mlp.b_out', 'blocks.10.ln1.w', 'blocks.10.ln1.b', 'blocks.10.ln2.w', 'blocks.10.ln2.b', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_O', 'blocks.10.attn.W_K', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_V', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.mlp.W_in', 'blocks.10.mlp.b_in', 'blocks.10.mlp.W_out', 'blocks.10.mlp.b_out', 'blocks.11.ln1.w', 'blocks.11.ln1.b', 'blocks.11.ln2.w', 'blocks.11.ln2.b', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_O', 'blocks.11.attn.W_K', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_V', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.mlp.W_in', 'blocks.11.mlp.b_in', 'blocks.11.mlp.W_out', 'blocks.11.mlp.b_out', 'ln_final.w', 'ln_final.b', 'unembed.W_U', 'unembed.b_U'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '568',\n",
       " '73',\n",
       " '+',\n",
       " '318',\n",
       " '46',\n",
       " '23',\n",
       " '=',\n",
       " '123',\n",
       " '45',\n",
       " '67',\n",
       " '89',\n",
       " '-',\n",
       " '1',\n",
       " '000000',\n",
       " '000']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='mps:0')\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens)\n",
    "# batch, position\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "# batch, position, d_vocab\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "print(log_probs.shape)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    zip(\n",
    "        reference_gpt2.to_str_tokens(reference_text),\n",
    "        reference_gpt2.tokenizer.batch_decode(\n",
    "            logits.argmax(dim=-1)[0]\n",
    "        ),  # Sample largest logit\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(314, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Map distribution to a token\n",
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0,   314]], device='mps:0')\n",
      "torch.Size([1, 36])\n",
      "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n",
      "torch.Size([1, 36, 50257])\n",
      "tensor(716, device='mps:0')\n",
      " am\n"
     ]
    }
   ],
   "source": [
    "next_tokens = torch.cat([tokens, next_token.clone().detach()[None, None]], dim=-1)\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "print(\"New Input:\", next_tokens)\n",
    "print(next_tokens.shape)\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
    "\n",
    "print(new_logits.shape)\n",
    "print(new_logits[-1, -1].argmax(-1))\n",
    "\n",
    "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 35, 768])\n",
      "hook_pos_embed torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
      "ln_final.hook_scale torch.Size([1, 35, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(activation_name, activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "blocks.0.ln1.w torch.Size([768])\n",
      "blocks.0.ln1.b torch.Size([768])\n",
      "blocks.0.ln2.w torch.Size([768])\n",
      "blocks.0.ln2.b torch.Size([768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "ln_final.w torch.Size([768])\n",
      "ln_final.b torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='mps'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768  # Embedding dimensions\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257  # Token dictionary size\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024  # Context length\n",
    "    d_head: int = 64  # Dimensions of each attention head, d_model // n_heads\n",
    "    d_mlp: int = 3072  # MLP hidden layer dimensions, d_model * 4\n",
    "    n_heads: int = 12  # Attention heads per block\n",
    "    n_layers: int = 12  # Number of transformer blocks\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    reference_output = gpt2_layer(reference_input)\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: torch.Tensor):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(\n",
    "            residual, \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )  # (batch, position, 1)\n",
    "        variance = einops.reduce(\n",
    "            residual.pow(2), \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )\n",
    "\n",
    "        normalized = (residual - mean) / (\n",
    "            variance + self.cfg.layer_norm_eps\n",
    "        ).sqrt()  # (batch, position, d_model)\n",
    "        out = normalized * self.w + self.b  # (batch, position, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "99.99% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_E[tokens]  # [batch, position, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_pos[\n",
    "            torch.arange(tokens.shape[1], device=device)  # [0, 1, 2, ..., position-1]\n",
    "        ]  # (position, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "          2.8267e-02,  5.4490e-02],\n",
       "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "          1.0172e-02, -1.5573e-04],\n",
       "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "          1.9325e-02, -2.1424e-02],\n",
       "        ...,\n",
       "        [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
       "         -2.3037e-03, -4.3189e-03],\n",
       "        [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
       "         -4.8160e-03, -9.2252e-04],\n",
       "        [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
       "         -3.2512e-03, -9.6509e-04]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
