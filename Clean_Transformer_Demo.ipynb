{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [What is a Transformer?](https://youtu.be/bOYE6E8JrtU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm\n",
    "import datasets\n",
    "import transformers\n",
    "import plotly.express as px\n",
    "# from unseal import transformers_util as tutil\n",
    "# from unseal import hooks\n",
    "# import unseal.visuals.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    # else \"mps\"\n",
    "    # if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gge/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseal_gpt2, unseal_tokenizer, unseal_config = tutil.load_from_pretrained(model_name)\n",
    "# unseal_gpt2.to(device)\n",
    "# hooked_gpt2 = hooks.HookedModel(unseal_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,\n",
      "          6067,     0]])\n",
      "tensor([[15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,  6067,\n",
      "             0]])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\")\n",
    ")\n",
    "print(\n",
    "    reference_gpt2.to_tokens(\n",
    "        \"Whether a word begins with a capital or space matters!\", prepend_bos=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed.W_E', 'pos_embed.W_pos', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.mlp.W_in', 'blocks.4.mlp.b_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_out', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.mlp.W_in', 'blocks.5.mlp.b_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.b_out', 'blocks.6.ln1.w', 'blocks.6.ln1.b', 'blocks.6.ln2.w', 'blocks.6.ln2.b', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_O', 'blocks.6.attn.W_K', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_V', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.mlp.W_in', 'blocks.6.mlp.b_in', 'blocks.6.mlp.W_out', 'blocks.6.mlp.b_out', 'blocks.7.ln1.w', 'blocks.7.ln1.b', 'blocks.7.ln2.w', 'blocks.7.ln2.b', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_O', 'blocks.7.attn.W_K', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_V', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.mlp.W_in', 'blocks.7.mlp.b_in', 'blocks.7.mlp.W_out', 'blocks.7.mlp.b_out', 'blocks.8.ln1.w', 'blocks.8.ln1.b', 'blocks.8.ln2.w', 'blocks.8.ln2.b', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_O', 'blocks.8.attn.W_K', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_V', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.mlp.W_in', 'blocks.8.mlp.b_in', 'blocks.8.mlp.W_out', 'blocks.8.mlp.b_out', 'blocks.9.ln1.w', 'blocks.9.ln1.b', 'blocks.9.ln2.w', 'blocks.9.ln2.b', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_O', 'blocks.9.attn.W_K', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_V', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.mlp.W_in', 'blocks.9.mlp.b_in', 'blocks.9.mlp.W_out', 'blocks.9.mlp.b_out', 'blocks.10.ln1.w', 'blocks.10.ln1.b', 'blocks.10.ln2.w', 'blocks.10.ln2.b', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_O', 'blocks.10.attn.W_K', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_V', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.mlp.W_in', 'blocks.10.mlp.b_in', 'blocks.10.mlp.W_out', 'blocks.10.mlp.b_out', 'blocks.11.ln1.w', 'blocks.11.ln1.b', 'blocks.11.ln2.w', 'blocks.11.ln2.b', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_O', 'blocks.11.attn.W_K', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_V', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.mlp.W_in', 'blocks.11.mlp.b_in', 'blocks.11.mlp.W_out', 'blocks.11.mlp.b_out', 'ln_final.w', 'ln_final.b', 'unembed.W_U', 'unembed.b_U'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '568',\n",
       " '73',\n",
       " '+',\n",
       " '318',\n",
       " '46',\n",
       " '23',\n",
       " '=',\n",
       " '123',\n",
       " '45',\n",
       " '67',\n",
       " '89',\n",
       " '-',\n",
       " '1',\n",
       " '000000',\n",
       " '000']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]])\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens)\n",
    "# batch, position\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "# batch, position, d_vocab\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "print(log_probs.shape)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    zip(\n",
    "        reference_gpt2.to_str_tokens(reference_text),\n",
    "        reference_gpt2.tokenizer.batch_decode(\n",
    "            logits.argmax(dim=-1)[0]\n",
    "        ),  # Sample largest logit\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(314)\n"
     ]
    }
   ],
   "source": [
    "# Map distribution to a token\n",
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0,   314]])\n",
      "torch.Size([1, 36])\n",
      "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n",
      "torch.Size([1, 36, 50257])\n",
      "tensor(716)\n",
      " am\n"
     ]
    }
   ],
   "source": [
    "next_tokens = torch.cat([tokens, next_token.clone().detach()[None, None]], dim=-1)\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "print(\"New Input:\", next_tokens)\n",
    "print(next_tokens.shape)\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
    "\n",
    "print(new_logits.shape)\n",
    "print(new_logits[-1, -1].argmax(-1))\n",
    "\n",
    "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 35, 768])\n",
      "hook_pos_embed torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
      "ln_final.hook_scale torch.Size([1, 35, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(activation_name, activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "blocks.0.ln1.w torch.Size([768])\n",
      "blocks.0.ln1.b torch.Size([768])\n",
      "blocks.0.ln2.w torch.Size([768])\n",
      "blocks.0.ln2.b torch.Size([768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "ln_final.w torch.Size([768])\n",
      "ln_final.b torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cpu'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768  # Embedding dimensions\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257  # Token dictionary size\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024  # Context length\n",
    "    d_head: int = 64  # Dimensions of each attention head, d_model // n_heads\n",
    "    d_mlp: int = 3072  # MLP hidden layer dimensions, d_model * 4\n",
    "    n_heads: int = 12  # Attention heads per block\n",
    "    n_layers: int = 12  # Number of transformer blocks\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_gpt2_test(\n",
    "    cls,\n",
    "    gpt2_layer,\n",
    "    input_name,\n",
    "    cache_dict=cache.cache_dict,\n",
    "    argcount=False,\n",
    "):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Modify this part to handle the different input requirements\n",
    "    if argcount:\n",
    "        # If the gpt2_layer expects separate inputs for query, key, and value\n",
    "        reference_output = gpt2_layer(reference_input, reference_input, reference_input)\n",
    "    else:\n",
    "        # If the gpt2_layer expects a single input\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: torch.Tensor):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(\n",
    "            residual, \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )  # (batch, position, 1)\n",
    "        variance = einops.reduce(\n",
    "            residual.pow(2), \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )\n",
    "\n",
    "        normalized = (residual - mean) / (\n",
    "            variance + self.cfg.layer_norm_eps\n",
    "        ).sqrt()  # (batch, position, d_model)\n",
    "        out = normalized * self.w + self.b  # (batch, position, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "99.99% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_E[tokens]  # [batch, position, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_pos[\n",
    "            torch.arange(tokens.shape[1], device=device)  # [0, 1, 2, ..., position-1]\n",
    "        ]  # (position, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "          2.8267e-02,  5.4490e-02],\n",
       "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "          1.0172e-02, -1.5573e-04],\n",
       "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "          1.9325e-02, -2.1424e-02],\n",
       "        ...,\n",
       "        [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
       "         -2.3037e-03, -4.3189e-03],\n",
       "        [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
       "         -4.8160e-03, -9.2252e-04],\n",
       "        [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
       "         -3.2512e-03, -9.6509e-04]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers = tutil.get_num_layers(hooked_gpt2, layer_key_prefix=\"transformer->h\")\n",
    "# html_storage = dict()\n",
    "# html_storage = utils.compute_attn_logits(\n",
    "#     hooked_gpt2,\n",
    "#     model_name,\n",
    "#     unseal_tokenizer,\n",
    "#     num_layers,\n",
    "#     reference_text,\n",
    "#     html_storage,\n",
    "#     layer_key_prefix=\"transformer->h\",\n",
    "#     out_proj_name=\"c_proj\",\n",
    "#     batch_size=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_pre: torch.Tensor):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        q = (\n",
    "            einsum(\n",
    "                \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "\n",
    "        scores = einsum(\n",
    "            \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\",\n",
    "            q,\n",
    "            k,\n",
    "        )\n",
    "        scores = scores / math.sqrt(self.cfg.d_head)\n",
    "        scores = self.apply_causal_mask(scores)\n",
    "\n",
    "        scores = scores.softmax(dim=-1)  # (batch, n_head, query_pos, key_pos)\n",
    "\n",
    "        v = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        heads_output = einsum(\n",
    "            \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\",\n",
    "            scores,\n",
    "            v,\n",
    "        )\n",
    "\n",
    "        attn_output = (\n",
    "            einsum(\n",
    "                \"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\",\n",
    "                heads_output,\n",
    "                self.W_O,\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores: torch.Tensor):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(\n",
    "            torch.ones(\n",
    "                attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device\n",
    "            ),\n",
    "            diagonal=1,\n",
    "        ).bool()\n",
    "        attn_scores.masked_fill_(mask, float(\"-inf\"))\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9663e-01,  1.6985e-02,  3.4781e-02,  ...,  3.3120e-02,\n",
       "          -2.3129e-02,  1.8103e-01],\n",
       "         [ 1.3168e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1998e-03,\n",
       "           5.3075e-03,  1.3511e-01],\n",
       "         [ 8.9737e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
       "           2.7958e-03,  9.0785e-02],\n",
       "         ...,\n",
       "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
       "          -4.9522e-04, -8.6008e-03],\n",
       "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6689e-02,\n",
       "          -1.4368e-02,  3.3245e-02],\n",
       "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
       "          -8.9027e-03,  4.8796e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    Attention,\n",
    "    reference_gpt2.blocks[0].attn,\n",
    "    cache[\"blocks.0.ln1.hook_normalized\"],\n",
    "    argcount=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_mid: torch.Tensor):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        hidden_layer = (\n",
    "            einsum(\n",
    "                \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        activation = gelu_new(hidden_layer)\n",
    "        output_layer = (\n",
    "            einsum(\n",
    "                \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "                activation,\n",
    "                self.W_out,\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
       "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
       "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
       "         ...,\n",
       "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
       "         [-0.0862,  0.7839,  0.9046,  ..., -0.2174, -0.5953,  0.8555],\n",
       "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre: torch.Tensor):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "\n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "96.70% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3911,  0.1542,  0.6004,  ...,  1.7199,  1.7365,  0.3928],\n",
       "         [-0.9035, -0.0360,  0.2350,  ..., -0.4147,  0.3562,  0.3932],\n",
       "         [-0.9644, -2.4814, -1.4994,  ...,  1.4044,  0.7617,  0.5915],\n",
       "         ...,\n",
       "         [-0.7420,  0.9250, -0.3219,  ...,  0.2920,  0.1097, -0.5344],\n",
       "         [-1.3221,  0.8959,  1.1793,  ..., -0.5544, -0.4071,  0.9254],\n",
       "         [ 1.1209, -0.8919,  1.3737,  ..., -0.1356,  0.3435,  0.4517]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(self, normalized_resid_final: torch.Tensor):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        logits = (\n",
    "            einsum(\n",
    "                \"batch position d_model, d_model d_vocab -> batch position d_vocab\",\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8365,  -43.0660,  ...,  -54.0878,  -54.3452,\n",
       "           -42.3645],\n",
       "         [-128.0392, -127.9936, -130.7011,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0063, -123.8819,  ..., -128.5180, -126.6027,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7750, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6725, -104.4889, -108.7362,  ..., -118.3553, -113.8767,\n",
       "          -106.3604],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        residual = embed + pos_embed\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        normalized_resid_final = self.ln_final(residual)\n",
    "        logits = self.unembed(normalized_resid_final)\n",
    "        # logits have shape [batch, position, logits]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4543,  -39.8557,  -43.0849,  ...,  -54.1043,  -54.3628,\n",
       "           -42.3845],\n",
       "         [-128.0192, -127.9733, -130.6802,  ..., -136.6926, -129.9065,\n",
       "          -129.3768],\n",
       "         [-119.8427, -120.9960, -123.8709,  ..., -128.5087, -126.5927,\n",
       "          -121.8961],\n",
       "         ...,\n",
       "         [-112.9791, -112.7720, -117.0613,  ..., -121.2889, -117.6548,\n",
       "          -114.4980],\n",
       "         [ -98.6525, -104.4702, -108.7174,  ..., -118.3360, -113.8571,\n",
       "          -106.3411],\n",
       "         [-126.8190, -128.9513, -128.3855,  ..., -140.1897, -138.5799,\n",
       "          -122.3602]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False))\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"Mini scule is a species of microhylid frog endemic to Madagascar that was described in 2019. The scientific name of the species refers to its size, being a pun on the word minuscule. It is very small, measuring only 8.4 to 10.8 mm (0.33 to 0.43 in) in snoutâ€“vent length. It has bronze underparts with a brown groin and back of the thigh, cream upperparts with brown flecking, a dark brown side of the head, and a red iris. On the hind feet, the first toe is absent and the second and fifth toes are strongly reduced. The frog is known only from the Sainte Luce Reserve, where it inhabits areas with deep leaf litter near semi-permanent water bodies. Specimens of frogs from Mandena, the Vohimena mountains, the southern Anosy Mountains, and Tsitongambarika may also be of this species. Along with Mini mum and Mini ature, the other two species in its genus, it received media attention when first described due to the wordplay in its scientific name. (Full article...)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "demo_logits = demo_gpt2(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7185, grad_fn=<NegBackward0>)\n",
      "Loss as average prob tensor(0.0243, grad_fn=<ExpBackward0>)\n",
      "Loss as 'uniform over this many variables' tensor(41.2019, grad_fn=<ExpBackward0>)\n",
      "Uniform loss over the vocab 10.82490511970208\n"
     ]
    }
   ],
   "source": [
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = (\n",
    "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    )\n",
    "    return -pred_log_probs.mean()\n",
    "\n",
    "\n",
    "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
    "print(loss)\n",
    "print(\"Loss as average prob\", (-loss).exp())\n",
    "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93eb05047c274cc0884fffc9783746c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The Senate is expected to begin the trial on Monday.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on\"\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1000\n",
    "log_every = 10\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "model_cfg = Config(\n",
    "    debug=False,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=reference_gpt2.cfg.d_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cebe0c9819e47f4838e15e86792b953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/373 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159ecefc82874c21985f0b50fc18acdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/921 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002ee36c4e2d418980fa0cbb0edd9ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb747179fddf4db1af04a3b16ab8379c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'meta'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web. Playi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9f0bca81d44acdab7be1db4beae619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80023 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (101051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (155995 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (229134 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "print(dataset)\n",
    "print(dataset[0][\"text\"][:100])\n",
    "tokens_dataset = tokenize_and_concatenate(\n",
    "    dataset,\n",
    "    reference_gpt2.tokenizer,\n",
    "    streaming=False,\n",
    "    max_length=model_cfg.n_ctx,\n",
    "    column_name=\"text\",\n",
    "    add_bos_token=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 8506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a70d4a2c274252b9e28c98b1d424c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 10.8545\n",
      "Step: 10, Loss: 8.6463\n",
      "Step: 20, Loss: 7.8609\n",
      "Step: 30, Loss: 7.4820\n",
      "Step: 40, Loss: 6.8126\n",
      "Step: 50, Loss: 7.8699\n",
      "Step: 60, Loss: 6.7977\n",
      "Step: 70, Loss: 7.7626\n",
      "Step: 80, Loss: 7.1344\n",
      "Step: 90, Loss: 6.0094\n",
      "Step: 100, Loss: 7.1833\n",
      "Step: 110, Loss: 7.6225\n",
      "Step: 120, Loss: 7.0261\n",
      "Step: 130, Loss: 6.2090\n",
      "Step: 140, Loss: 6.2792\n",
      "Step: 150, Loss: 7.3507\n",
      "Step: 160, Loss: 5.8855\n",
      "Step: 170, Loss: 7.4611\n",
      "Step: 180, Loss: 6.6623\n",
      "Step: 190, Loss: 5.6294\n",
      "Step: 200, Loss: 7.1838\n",
      "Step: 210, Loss: 6.4661\n",
      "Step: 220, Loss: 7.5148\n",
      "Step: 230, Loss: 7.2182\n",
      "Step: 240, Loss: 6.2784\n",
      "Step: 250, Loss: 6.4560\n",
      "Step: 260, Loss: 6.6954\n",
      "Step: 270, Loss: 6.6621\n",
      "Step: 280, Loss: 6.7842\n",
      "Step: 290, Loss: 6.0734\n",
      "Step: 300, Loss: 6.8871\n",
      "Step: 310, Loss: 6.5101\n",
      "Step: 320, Loss: 6.4092\n",
      "Step: 330, Loss: 6.6884\n",
      "Step: 340, Loss: 6.2512\n",
      "Step: 350, Loss: 5.8480\n",
      "Step: 360, Loss: 7.4350\n",
      "Step: 370, Loss: 7.2847\n",
      "Step: 380, Loss: 5.9220\n",
      "Step: 390, Loss: 6.4583\n",
      "Step: 400, Loss: 6.2268\n",
      "Step: 410, Loss: 7.0767\n",
      "Step: 420, Loss: 6.5245\n",
      "Step: 430, Loss: 6.5273\n",
      "Step: 440, Loss: 5.9617\n",
      "Step: 450, Loss: 6.6643\n",
      "Step: 460, Loss: 5.5368\n",
      "Step: 470, Loss: 5.9663\n",
      "Step: 480, Loss: 5.9991\n",
      "Step: 490, Loss: 6.5193\n",
      "Step: 500, Loss: 6.9081\n",
      "Step: 510, Loss: 5.5401\n",
      "Step: 520, Loss: 6.8353\n",
      "Step: 530, Loss: 6.4210\n",
      "Step: 540, Loss: 6.4170\n",
      "Step: 550, Loss: 6.3789\n",
      "Step: 560, Loss: 6.6912\n",
      "Step: 570, Loss: 6.1519\n",
      "Step: 580, Loss: 4.7031\n",
      "Step: 590, Loss: 5.8601\n",
      "Step: 600, Loss: 5.9003\n",
      "Step: 610, Loss: 4.8836\n",
      "Step: 620, Loss: 6.7487\n",
      "Step: 630, Loss: 6.6415\n",
      "Step: 640, Loss: 5.0321\n",
      "Step: 650, Loss: 6.1999\n",
      "Step: 660, Loss: 7.0955\n",
      "Step: 670, Loss: 5.5337\n",
      "Step: 680, Loss: 6.3381\n",
      "Step: 690, Loss: 5.3934\n",
      "Step: 700, Loss: 4.8369\n",
      "Step: 710, Loss: 6.0060\n",
      "Step: 720, Loss: 6.4816\n",
      "Step: 730, Loss: 5.6784\n",
      "Step: 740, Loss: 4.6981\n",
      "Step: 750, Loss: 5.8928\n",
      "Step: 760, Loss: 6.2577\n",
      "Step: 770, Loss: 6.0866\n",
      "Step: 780, Loss: 6.6630\n",
      "Step: 790, Loss: 5.6244\n",
      "Step: 800, Loss: 5.0472\n",
      "Step: 810, Loss: 6.1552\n",
      "Step: 820, Loss: 5.6351\n",
      "Step: 830, Loss: 5.4775\n",
      "Step: 840, Loss: 5.0154\n",
      "Step: 850, Loss: 6.6948\n",
      "Step: 860, Loss: 6.0825\n",
      "Step: 870, Loss: 5.8253\n",
      "Step: 880, Loss: 5.0729\n",
      "Step: 890, Loss: 6.0126\n",
      "Step: 900, Loss: 5.4577\n",
      "Step: 910, Loss: 5.9957\n",
      "Step: 920, Loss: 5.7635\n",
      "Step: 930, Loss: 5.4684\n",
      "Step: 940, Loss: 5.1013\n",
      "Step: 950, Loss: 4.9825\n",
      "Step: 960, Loss: 4.4775\n",
      "Step: 970, Loss: 6.7750\n",
      "Step: 980, Loss: 4.6651\n",
      "Step: 990, Loss: 5.6600\n",
      "Step: 1000, Loss: 6.0825\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "print(\"Number of batches:\", len(data_loader))\n",
    "for epoch in range(num_epochs):\n",
    "    for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "        tokens = batch[\"tokens\"]\n",
    "        logits = model(tokens)\n",
    "        loss = lm_cross_entropy_loss(logits, tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if c % log_every == 0:\n",
    "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "        if c > max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/IPython/core/formatters.py:925\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    923\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/plotly/basedatatypes.py:832\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mrender_on_display \u001b[38;5;129;01mand\u001b[39;00m pio\u001b[38;5;241m.\u001b[39mrenderers\u001b[38;5;241m.\u001b[39mdefault:\n\u001b[0;32m--> 832\u001b[0m     \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[0;32m~/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/plotly/io/_renderers.py:394\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         )\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    398\u001b[0m     ipython_display\u001b[38;5;241m.\u001b[39mdisplay(bundle, raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Tokens=%{x}<br>Loss=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "showlegend": false,
         "type": "scattergl",
         "x": [
          0,
          2048,
          4096,
          6144,
          8192,
          10240,
          12288,
          14336,
          16384,
          18432,
          20480,
          22528,
          24576,
          26624,
          28672,
          30720,
          32768,
          34816,
          36864,
          38912,
          40960,
          43008,
          45056,
          47104,
          49152,
          51200,
          53248,
          55296,
          57344,
          59392,
          61440,
          63488,
          65536,
          67584,
          69632,
          71680,
          73728,
          75776,
          77824,
          79872,
          81920,
          83968,
          86016,
          88064,
          90112,
          92160,
          94208,
          96256,
          98304,
          100352,
          102400,
          104448,
          106496,
          108544,
          110592,
          112640,
          114688,
          116736,
          118784,
          120832,
          122880,
          124928,
          126976,
          129024,
          131072,
          133120,
          135168,
          137216,
          139264,
          141312,
          143360,
          145408,
          147456,
          149504,
          151552,
          153600,
          155648,
          157696,
          159744,
          161792,
          163840,
          165888,
          167936,
          169984,
          172032,
          174080,
          176128,
          178176,
          180224,
          182272,
          184320,
          186368,
          188416,
          190464,
          192512,
          194560,
          196608,
          198656,
          200704,
          202752,
          204800,
          206848,
          208896,
          210944,
          212992,
          215040,
          217088,
          219136,
          221184,
          223232,
          225280,
          227328,
          229376,
          231424,
          233472,
          235520,
          237568,
          239616,
          241664,
          243712,
          245760,
          247808,
          249856,
          251904,
          253952,
          256000,
          258048,
          260096,
          262144,
          264192,
          266240,
          268288,
          270336,
          272384,
          274432,
          276480,
          278528,
          280576,
          282624,
          284672,
          286720,
          288768,
          290816,
          292864,
          294912,
          296960,
          299008,
          301056,
          303104,
          305152,
          307200,
          309248,
          311296,
          313344,
          315392,
          317440,
          319488,
          321536,
          323584,
          325632,
          327680,
          329728,
          331776,
          333824,
          335872,
          337920,
          339968,
          342016,
          344064,
          346112,
          348160,
          350208,
          352256,
          354304,
          356352,
          358400,
          360448,
          362496,
          364544,
          366592,
          368640,
          370688,
          372736,
          374784,
          376832,
          378880,
          380928,
          382976,
          385024,
          387072,
          389120,
          391168,
          393216,
          395264,
          397312,
          399360,
          401408,
          403456,
          405504,
          407552,
          409600,
          411648,
          413696,
          415744,
          417792,
          419840,
          421888,
          423936,
          425984,
          428032,
          430080,
          432128,
          434176,
          436224,
          438272,
          440320,
          442368,
          444416,
          446464,
          448512,
          450560,
          452608,
          454656,
          456704,
          458752,
          460800,
          462848,
          464896,
          466944,
          468992,
          471040,
          473088,
          475136,
          477184,
          479232,
          481280,
          483328,
          485376,
          487424,
          489472,
          491520,
          493568,
          495616,
          497664,
          499712,
          501760,
          503808,
          505856,
          507904,
          509952,
          512000,
          514048,
          516096,
          518144,
          520192,
          522240,
          524288,
          526336,
          528384,
          530432,
          532480,
          534528,
          536576,
          538624,
          540672,
          542720,
          544768,
          546816,
          548864,
          550912,
          552960,
          555008,
          557056,
          559104,
          561152,
          563200,
          565248,
          567296,
          569344,
          571392,
          573440,
          575488,
          577536,
          579584,
          581632,
          583680,
          585728,
          587776,
          589824,
          591872,
          593920,
          595968,
          598016,
          600064,
          602112,
          604160,
          606208,
          608256,
          610304,
          612352,
          614400,
          616448,
          618496,
          620544,
          622592,
          624640,
          626688,
          628736,
          630784,
          632832,
          634880,
          636928,
          638976,
          641024,
          643072,
          645120,
          647168,
          649216,
          651264,
          653312,
          655360,
          657408,
          659456,
          661504,
          663552,
          665600,
          667648,
          669696,
          671744,
          673792,
          675840,
          677888,
          679936,
          681984,
          684032,
          686080,
          688128,
          690176,
          692224,
          694272,
          696320,
          698368,
          700416,
          702464,
          704512,
          706560,
          708608,
          710656,
          712704,
          714752,
          716800,
          718848,
          720896,
          722944,
          724992,
          727040,
          729088,
          731136,
          733184,
          735232,
          737280,
          739328,
          741376,
          743424,
          745472,
          747520,
          749568,
          751616,
          753664,
          755712,
          757760,
          759808,
          761856,
          763904,
          765952,
          768000,
          770048,
          772096,
          774144,
          776192,
          778240,
          780288,
          782336,
          784384,
          786432,
          788480,
          790528,
          792576,
          794624,
          796672,
          798720,
          800768,
          802816,
          804864,
          806912,
          808960,
          811008,
          813056,
          815104,
          817152,
          819200,
          821248,
          823296,
          825344,
          827392,
          829440,
          831488,
          833536,
          835584,
          837632,
          839680,
          841728,
          843776,
          845824,
          847872,
          849920,
          851968,
          854016,
          856064,
          858112,
          860160,
          862208,
          864256,
          866304,
          868352,
          870400,
          872448,
          874496,
          876544,
          878592,
          880640,
          882688,
          884736,
          886784,
          888832,
          890880,
          892928,
          894976,
          897024,
          899072,
          901120,
          903168,
          905216,
          907264,
          909312,
          911360,
          913408,
          915456,
          917504,
          919552,
          921600,
          923648,
          925696,
          927744,
          929792,
          931840,
          933888,
          935936,
          937984,
          940032,
          942080,
          944128,
          946176,
          948224,
          950272,
          952320,
          954368,
          956416,
          958464,
          960512,
          962560,
          964608,
          966656,
          968704,
          970752,
          972800,
          974848,
          976896,
          978944,
          980992,
          983040,
          985088,
          987136,
          989184,
          991232,
          993280,
          995328,
          997376,
          999424,
          1001472,
          1003520,
          1005568,
          1007616,
          1009664,
          1011712,
          1013760,
          1015808,
          1017856,
          1019904,
          1021952,
          1024000,
          1026048,
          1028096,
          1030144,
          1032192,
          1034240,
          1036288,
          1038336,
          1040384,
          1042432,
          1044480,
          1046528,
          1048576,
          1050624,
          1052672,
          1054720,
          1056768,
          1058816,
          1060864,
          1062912,
          1064960,
          1067008,
          1069056,
          1071104,
          1073152,
          1075200,
          1077248,
          1079296,
          1081344,
          1083392,
          1085440,
          1087488,
          1089536,
          1091584,
          1093632,
          1095680,
          1097728,
          1099776,
          1101824,
          1103872,
          1105920,
          1107968,
          1110016,
          1112064,
          1114112,
          1116160,
          1118208,
          1120256,
          1122304,
          1124352,
          1126400,
          1128448,
          1130496,
          1132544,
          1134592,
          1136640,
          1138688,
          1140736,
          1142784,
          1144832,
          1146880,
          1148928,
          1150976,
          1153024,
          1155072,
          1157120,
          1159168,
          1161216,
          1163264,
          1165312,
          1167360,
          1169408,
          1171456,
          1173504,
          1175552,
          1177600,
          1179648,
          1181696,
          1183744,
          1185792,
          1187840,
          1189888,
          1191936,
          1193984,
          1196032,
          1198080,
          1200128,
          1202176,
          1204224,
          1206272,
          1208320,
          1210368,
          1212416,
          1214464,
          1216512,
          1218560,
          1220608,
          1222656,
          1224704,
          1226752,
          1228800,
          1230848,
          1232896,
          1234944,
          1236992,
          1239040,
          1241088,
          1243136,
          1245184,
          1247232,
          1249280,
          1251328,
          1253376,
          1255424,
          1257472,
          1259520,
          1261568,
          1263616,
          1265664,
          1267712,
          1269760,
          1271808,
          1273856,
          1275904,
          1277952,
          1280000,
          1282048,
          1284096,
          1286144,
          1288192,
          1290240,
          1292288,
          1294336,
          1296384,
          1298432,
          1300480,
          1302528,
          1304576,
          1306624,
          1308672,
          1310720,
          1312768,
          1314816,
          1316864,
          1318912,
          1320960,
          1323008,
          1325056,
          1327104,
          1329152,
          1331200,
          1333248,
          1335296,
          1337344,
          1339392,
          1341440,
          1343488,
          1345536,
          1347584,
          1349632,
          1351680,
          1353728,
          1355776,
          1357824,
          1359872,
          1361920,
          1363968,
          1366016,
          1368064,
          1370112,
          1372160,
          1374208,
          1376256,
          1378304,
          1380352,
          1382400,
          1384448,
          1386496,
          1388544,
          1390592,
          1392640,
          1394688,
          1396736,
          1398784,
          1400832,
          1402880,
          1404928,
          1406976,
          1409024,
          1411072,
          1413120,
          1415168,
          1417216,
          1419264,
          1421312,
          1423360,
          1425408,
          1427456,
          1429504,
          1431552,
          1433600,
          1435648,
          1437696,
          1439744,
          1441792,
          1443840,
          1445888,
          1447936,
          1449984,
          1452032,
          1454080,
          1456128,
          1458176,
          1460224,
          1462272,
          1464320,
          1466368,
          1468416,
          1470464,
          1472512,
          1474560,
          1476608,
          1478656,
          1480704,
          1482752,
          1484800,
          1486848,
          1488896,
          1490944,
          1492992,
          1495040,
          1497088,
          1499136,
          1501184,
          1503232,
          1505280,
          1507328,
          1509376,
          1511424,
          1513472,
          1515520,
          1517568,
          1519616,
          1521664,
          1523712,
          1525760,
          1527808,
          1529856,
          1531904,
          1533952,
          1536000,
          1538048,
          1540096,
          1542144,
          1544192,
          1546240,
          1548288,
          1550336,
          1552384,
          1554432,
          1556480,
          1558528,
          1560576,
          1562624,
          1564672,
          1566720,
          1568768,
          1570816,
          1572864,
          1574912,
          1576960,
          1579008,
          1581056,
          1583104,
          1585152,
          1587200,
          1589248,
          1591296,
          1593344,
          1595392,
          1597440,
          1599488,
          1601536,
          1603584,
          1605632,
          1607680,
          1609728,
          1611776,
          1613824,
          1615872,
          1617920,
          1619968,
          1622016,
          1624064,
          1626112,
          1628160,
          1630208,
          1632256,
          1634304,
          1636352,
          1638400,
          1640448,
          1642496,
          1644544,
          1646592,
          1648640,
          1650688,
          1652736,
          1654784,
          1656832,
          1658880,
          1660928,
          1662976,
          1665024,
          1667072,
          1669120,
          1671168,
          1673216,
          1675264,
          1677312,
          1679360,
          1681408,
          1683456,
          1685504,
          1687552,
          1689600,
          1691648,
          1693696,
          1695744,
          1697792,
          1699840,
          1701888,
          1703936,
          1705984,
          1708032,
          1710080,
          1712128,
          1714176,
          1716224,
          1718272,
          1720320,
          1722368,
          1724416,
          1726464,
          1728512,
          1730560,
          1732608,
          1734656,
          1736704,
          1738752,
          1740800,
          1742848,
          1744896,
          1746944,
          1748992,
          1751040,
          1753088,
          1755136,
          1757184,
          1759232,
          1761280,
          1763328,
          1765376,
          1767424,
          1769472,
          1771520,
          1773568,
          1775616,
          1777664,
          1779712,
          1781760,
          1783808,
          1785856,
          1787904,
          1789952,
          1792000,
          1794048,
          1796096,
          1798144,
          1800192,
          1802240,
          1804288,
          1806336,
          1808384,
          1810432,
          1812480,
          1814528,
          1816576,
          1818624,
          1820672,
          1822720,
          1824768,
          1826816,
          1828864,
          1830912,
          1832960,
          1835008,
          1837056,
          1839104,
          1841152,
          1843200,
          1845248,
          1847296,
          1849344,
          1851392,
          1853440,
          1855488,
          1857536,
          1859584,
          1861632,
          1863680,
          1865728,
          1867776,
          1869824,
          1871872,
          1873920,
          1875968,
          1878016,
          1880064,
          1882112,
          1884160,
          1886208,
          1888256,
          1890304,
          1892352,
          1894400,
          1896448,
          1898496,
          1900544,
          1902592,
          1904640,
          1906688,
          1908736,
          1910784,
          1912832,
          1914880,
          1916928,
          1918976,
          1921024,
          1923072,
          1925120,
          1927168,
          1929216,
          1931264,
          1933312,
          1935360,
          1937408,
          1939456,
          1941504,
          1943552,
          1945600,
          1947648,
          1949696,
          1951744,
          1953792,
          1955840,
          1957888,
          1959936,
          1961984,
          1964032,
          1966080,
          1968128,
          1970176,
          1972224,
          1974272,
          1976320,
          1978368,
          1980416,
          1982464,
          1984512,
          1986560,
          1988608,
          1990656,
          1992704,
          1994752,
          1996800,
          1998848,
          2000896,
          2002944,
          2004992,
          2007040,
          2009088,
          2011136,
          2013184,
          2015232,
          2017280,
          2019328,
          2021376,
          2023424,
          2025472,
          2027520,
          2029568,
          2031616,
          2033664,
          2035712,
          2037760,
          2039808,
          2041856,
          2043904,
          2045952,
          2048000,
          2050048
         ],
         "xaxis": "x",
         "y": [
          10.854450225830078,
          10.463028907775879,
          10.383688926696777,
          9.776139259338379,
          9.935223579406738,
          9.540785789489746,
          9.517547607421875,
          8.807174682617188,
          8.66366958618164,
          8.67220401763916,
          8.646254539489746,
          8.42113971710205,
          7.951788425445557,
          8.28551197052002,
          7.5558857917785645,
          8.289531707763672,
          7.331459999084473,
          7.155359268188477,
          6.948245048522949,
          7.259580612182617,
          7.860927104949951,
          8.002614974975586,
          8.191458702087402,
          8.62995719909668,
          6.445733070373535,
          6.918249607086182,
          5.920351028442383,
          7.665004253387451,
          8.17246150970459,
          6.877509117126465,
          7.481996059417725,
          8.20982551574707,
          8.243618965148926,
          7.20925235748291,
          7.682490825653076,
          7.564950942993164,
          7.827813625335693,
          7.9076457023620605,
          5.502355575561523,
          8.095667839050293,
          6.812557220458984,
          7.712421417236328,
          6.73745584487915,
          7.168408393859863,
          8.01118278503418,
          8.143072128295898,
          7.743032455444336,
          6.729619026184082,
          6.296523571014404,
          6.803936958312988,
          7.869935035705566,
          7.378191947937012,
          7.625899314880371,
          8.260580062866211,
          7.043929576873779,
          7.457998752593994,
          7.156436443328857,
          7.0353546142578125,
          6.438826084136963,
          7.5832200050354,
          6.797715663909912,
          6.61657190322876,
          6.887738227844238,
          7.793162822723389,
          7.819830417633057,
          5.485851287841797,
          7.441709041595459,
          6.917121887207031,
          5.862812042236328,
          6.9526190757751465,
          7.7625603675842285,
          7.358996868133545,
          7.848251819610596,
          7.678569316864014,
          7.508083343505859,
          6.041210174560547,
          7.231173515319824,
          7.069821357727051,
          7.280869960784912,
          7.8199944496154785,
          7.134366512298584,
          6.509840488433838,
          7.917928695678711,
          7.603498458862305,
          7.646491050720215,
          7.432402610778809,
          7.537469863891602,
          6.594364643096924,
          6.112136363983154,
          7.6856160163879395,
          6.009410381317139,
          6.745038032531738,
          7.4962592124938965,
          7.712955951690674,
          7.570219993591309,
          6.636895656585693,
          7.595190048217773,
          6.400562286376953,
          7.5872416496276855,
          7.519845962524414,
          7.183290958404541,
          7.42397403717041,
          7.816910266876221,
          7.317833423614502,
          7.682782173156738,
          7.112697601318359,
          7.511951923370361,
          7.661698818206787,
          6.5653862953186035,
          7.605839252471924,
          7.622527122497559,
          6.800365447998047,
          7.028132438659668,
          6.904405117034912,
          6.619532108306885,
          7.835614204406738,
          7.360959053039551,
          6.969531059265137,
          7.386767864227295,
          5.361082077026367,
          7.026078224182129,
          6.2272562980651855,
          7.423372268676758,
          7.476095199584961,
          6.431253433227539,
          7.564887046813965,
          7.296453952789307,
          6.215089797973633,
          7.22170352935791,
          6.807578086853027,
          6.208980560302734,
          6.2713704109191895,
          7.362666130065918,
          7.297800064086914,
          6.010257244110107,
          7.495741367340088,
          7.115799427032471,
          6.621230125427246,
          7.421770095825195,
          6.946279048919678,
          6.279150009155273,
          6.676866054534912,
          7.226798057556152,
          6.858416557312012,
          7.5043158531188965,
          6.529477596282959,
          7.343371868133545,
          7.496452808380127,
          6.365985870361328,
          6.2831034660339355,
          7.350709438323975,
          7.375933647155762,
          6.78054666519165,
          6.12939977645874,
          7.844069957733154,
          6.7953104972839355,
          6.250616550445557,
          6.462862491607666,
          6.696166038513184,
          6.892684459686279,
          5.885461807250977,
          6.652431488037109,
          7.249911308288574,
          7.267889499664307,
          7.115579605102539,
          7.090950965881348,
          7.832364082336426,
          6.951417922973633,
          7.528704643249512,
          7.597931861877441,
          7.461143970489502,
          7.205909252166748,
          6.815666198730469,
          6.47885799407959,
          7.524909019470215,
          6.258906841278076,
          6.895514488220215,
          6.964444160461426,
          6.179443359375,
          7.5113420486450195,
          6.662262439727783,
          6.91325044631958,
          6.452484607696533,
          5.9713969230651855,
          7.58229923248291,
          7.09904670715332,
          6.2139387130737305,
          7.296949863433838,
          6.415127754211426,
          7.032045364379883,
          5.629449844360352,
          6.864357948303223,
          7.072058200836182,
          7.470970153808594,
          6.9340996742248535,
          6.087735652923584,
          7.016051292419434,
          6.916079044342041,
          7.58107328414917,
          7.1422438621521,
          7.183753490447998,
          6.901201248168945,
          7.138371467590332,
          6.669116973876953,
          6.936522483825684,
          5.085166931152344,
          4.692669868469238,
          6.803583145141602,
          6.952923774719238,
          6.505000591278076,
          6.466141223907471,
          7.350580215454102,
          7.41052770614624,
          6.603331089019775,
          6.825038433074951,
          6.291366100311279,
          7.19889497756958,
          6.922217845916748,
          6.664485931396484,
          7.409808158874512,
          7.514760494232178,
          7.234734535217285,
          7.350885391235352,
          6.564659118652344,
          7.0379862785339355,
          6.2766547203063965,
          6.621244430541992,
          6.42045783996582,
          6.09067964553833,
          5.8518571853637695,
          7.218234062194824,
          5.062824726104736,
          6.467580318450928,
          7.173182964324951,
          7.05493688583374,
          6.943305492401123,
          7.260612487792969,
          7.41037654876709,
          6.722615718841553,
          7.32243013381958,
          6.278411865234375,
          6.666016578674316,
          6.746776103973389,
          6.519835472106934,
          5.940471172332764,
          6.506335258483887,
          6.1811113357543945,
          6.053864002227783,
          7.578948020935059,
          7.1554274559021,
          6.4559502601623535,
          7.057476997375488,
          6.821982383728027,
          6.645841121673584,
          6.899338722229004,
          7.183646202087402,
          7.183658599853516,
          6.009227275848389,
          6.7261643409729,
          6.572842597961426,
          6.695363998413086,
          6.398828506469727,
          6.645105838775635,
          6.391038417816162,
          6.849864482879639,
          6.943141460418701,
          5.635378360748291,
          6.468249320983887,
          5.875392436981201,
          6.488244533538818,
          6.66205358505249,
          7.511533260345459,
          5.729803562164307,
          5.562570571899414,
          7.2957763671875,
          7.138801097869873,
          6.775303840637207,
          7.396947383880615,
          6.4913811683654785,
          6.554899215698242,
          6.784169673919678,
          7.1439032554626465,
          6.045857906341553,
          6.638937950134277,
          7.2307000160217285,
          5.966411113739014,
          6.997823715209961,
          7.074563026428223,
          6.1482253074646,
          6.262328624725342,
          6.073430061340332,
          6.794606685638428,
          6.065927505493164,
          4.977292060852051,
          6.78422737121582,
          7.097419261932373,
          6.059145450592041,
          4.185710906982422,
          6.566164970397949,
          7.128326416015625,
          6.887077331542969,
          6.601691246032715,
          6.861466407775879,
          6.752424716949463,
          7.457740306854248,
          5.896881580352783,
          5.89732551574707,
          6.059147834777832,
          6.363085746765137,
          6.01101541519165,
          6.510138511657715,
          6.312315940856934,
          7.058523178100586,
          6.380595684051514,
          6.561495780944824,
          6.211428165435791,
          6.913214683532715,
          6.152157783508301,
          6.933550834655762,
          7.3156232833862305,
          6.40919828414917,
          6.5238518714904785,
          6.801835536956787,
          7.238858699798584,
          6.506946086883545,
          7.298291206359863,
          6.940350532531738,
          4.696794509887695,
          6.194520950317383,
          6.555710792541504,
          6.688409805297852,
          6.030770301818848,
          6.032138824462891,
          6.5558061599731445,
          5.7547197341918945,
          4.9841694831848145,
          6.733280181884766,
          7.1334452629089355,
          7.132935523986816,
          5.98329496383667,
          6.251224040985107,
          6.1005401611328125,
          5.606653213500977,
          5.370863914489746,
          7.127809524536133,
          5.751622676849365,
          7.168690204620361,
          7.13054895401001,
          6.764644145965576,
          6.809328556060791,
          5.847984790802002,
          6.864226818084717,
          6.547879219055176,
          6.492173194885254,
          6.84027624130249,
          5.860219955444336,
          4.866832733154297,
          6.342408657073975,
          5.843356609344482,
          6.596684455871582,
          7.435032844543457,
          6.478963851928711,
          6.323365688323975,
          6.918969631195068,
          6.3177995681762695,
          6.713742733001709,
          6.843751430511475,
          6.512274742126465,
          6.446118354797363,
          6.9718098640441895,
          7.284698486328125,
          5.909122943878174,
          6.8837175369262695,
          6.591510772705078,
          6.729146957397461,
          6.964982986450195,
          6.496385097503662,
          7.100907802581787,
          6.68963098526001,
          5.974977493286133,
          5.922025680541992,
          6.133410453796387,
          6.02763032913208,
          6.242808818817139,
          6.801566123962402,
          6.399755001068115,
          6.366701602935791,
          6.995099067687988,
          5.918675422668457,
          6.6862640380859375,
          6.45829439163208,
          5.880483627319336,
          6.6393818855285645,
          5.778737545013428,
          6.828270435333252,
          6.008747577667236,
          5.958245277404785,
          6.924577713012695,
          6.0521626472473145,
          7.347006320953369,
          6.226801872253418,
          6.219110012054443,
          6.761191368103027,
          6.599098205566406,
          6.150038719177246,
          6.075898170471191,
          6.9524760246276855,
          5.565514087677002,
          5.938342571258545,
          6.821135997772217,
          7.076663017272949,
          5.66713285446167,
          6.317336082458496,
          5.914614677429199,
          7.116015434265137,
          6.374999046325684,
          5.8701863288879395,
          7.394593238830566,
          5.239419460296631,
          6.751798629760742,
          6.524456024169922,
          6.689646244049072,
          6.317525863647461,
          7.033802509307861,
          6.637807369232178,
          6.574493885040283,
          7.198011875152588,
          6.692473888397217,
          6.262240886688232,
          5.771246433258057,
          6.527289390563965,
          6.774779319763184,
          6.538018703460693,
          6.132085800170898,
          5.728128910064697,
          6.366733551025391,
          5.733404159545898,
          6.929437637329102,
          5.862786769866943,
          4.678160190582275,
          5.961729049682617,
          5.501026153564453,
          5.150195121765137,
          6.614116668701172,
          5.832125186920166,
          5.766794204711914,
          5.465226650238037,
          6.625421047210693,
          6.925655364990234,
          5.964712619781494,
          6.664275169372559,
          5.63389778137207,
          5.943923473358154,
          7.198044776916504,
          6.581083297729492,
          6.3163981437683105,
          5.9027910232543945,
          6.156276226043701,
          5.892621040344238,
          6.4538068771362305,
          5.536754131317139,
          4.611171245574951,
          5.5808916091918945,
          6.369757175445557,
          5.02683162689209,
          6.983375072479248,
          6.369698524475098,
          6.717695236206055,
          6.445165157318115,
          6.606196403503418,
          5.966333389282227,
          6.132497310638428,
          5.9362101554870605,
          6.412680625915527,
          6.387152671813965,
          6.509056091308594,
          6.136280059814453,
          6.5208420753479,
          7.014150142669678,
          7.141970634460449,
          5.999146938323975,
          6.3928656578063965,
          6.779301643371582,
          5.963932037353516,
          6.876800060272217,
          6.44705057144165,
          6.328127861022949,
          6.3202691078186035,
          5.52315616607666,
          5.674781322479248,
          6.519289970397949,
          5.665248870849609,
          6.407644748687744,
          6.916439056396484,
          6.315254211425781,
          5.883701801300049,
          6.401333808898926,
          5.865872383117676,
          6.398603916168213,
          6.40331506729126,
          6.908053874969482,
          6.5544586181640625,
          6.333073616027832,
          6.23280668258667,
          6.893930912017822,
          6.121990203857422,
          6.394949436187744,
          6.506539821624756,
          5.809802055358887,
          7.005299091339111,
          5.540140628814697,
          6.208792209625244,
          6.254007816314697,
          6.500553131103516,
          5.597997188568115,
          6.820410251617432,
          5.953258514404297,
          5.752680778503418,
          5.9722394943237305,
          6.3845295906066895,
          6.835316181182861,
          6.545851707458496,
          5.7922234535217285,
          5.978375434875488,
          4.608325958251953,
          6.76346492767334,
          5.7530198097229,
          6.517764091491699,
          4.347540378570557,
          6.52103328704834,
          6.421048641204834,
          6.133098125457764,
          6.979004383087158,
          5.200563907623291,
          6.642823219299316,
          6.166574001312256,
          6.542239189147949,
          5.717117786407471,
          6.0630717277526855,
          6.383273124694824,
          6.416958332061768,
          6.679996013641357,
          6.192503929138184,
          6.440696716308594,
          6.4084320068359375,
          6.63875675201416,
          6.894071102142334,
          6.484639644622803,
          5.7469916343688965,
          4.701140880584717,
          6.3789381980896,
          6.194981098175049,
          6.588928699493408,
          6.027205944061279,
          5.9799346923828125,
          5.648458480834961,
          5.312124729156494,
          6.874805450439453,
          6.776154518127441,
          5.820784568786621,
          6.691176891326904,
          7.0295000076293945,
          5.611313343048096,
          5.978029251098633,
          6.88893985748291,
          6.821957111358643,
          5.626959323883057,
          5.345090389251709,
          5.454472541809082,
          5.980249881744385,
          6.151882648468018,
          6.171768665313721,
          7.049464225769043,
          6.269687175750732,
          6.841146469116211,
          6.155098915100098,
          6.370288848876953,
          6.1082305908203125,
          6.549829006195068,
          5.592128276824951,
          4.703105926513672,
          5.816777229309082,
          5.977025985717773,
          6.0030412673950195,
          5.4569091796875,
          5.84878396987915,
          5.921636581420898,
          6.190796375274658,
          6.701752662658691,
          6.464745044708252,
          5.8600754737854,
          6.319988250732422,
          5.792882919311523,
          5.924829483032227,
          6.348631858825684,
          5.987783908843994,
          5.815739154815674,
          5.870761394500732,
          7.056538105010986,
          6.126522064208984,
          5.90033483505249,
          6.00835657119751,
          6.297720909118652,
          5.361133575439453,
          5.149258613586426,
          6.567466735839844,
          6.071683406829834,
          5.817484378814697,
          6.720035076141357,
          5.414102554321289,
          4.883605003356934,
          6.064277172088623,
          6.766464710235596,
          5.311984062194824,
          6.905226230621338,
          6.266335487365723,
          5.266031265258789,
          6.861542701721191,
          6.88754415512085,
          5.31223726272583,
          6.748654842376709,
          6.801799297332764,
          6.83816385269165,
          6.678792476654053,
          5.829503059387207,
          5.7914605140686035,
          6.736681938171387,
          6.98419713973999,
          6.938019275665283,
          5.815917491912842,
          6.641533374786377,
          6.316003799438477,
          6.168960094451904,
          5.220558166503906,
          6.5882039070129395,
          6.5039963722229,
          5.6971940994262695,
          6.511314868927002,
          6.278578758239746,
          6.029819488525391,
          5.032116889953613,
          6.294021129608154,
          6.550290584564209,
          7.071104049682617,
          6.35819673538208,
          6.800181865692139,
          5.4153032302856445,
          6.54008674621582,
          6.549932956695557,
          5.829015254974365,
          6.199897766113281,
          6.227444648742676,
          6.703969478607178,
          6.208791732788086,
          6.214728355407715,
          6.553897857666016,
          6.042068004608154,
          5.225064277648926,
          5.877416133880615,
          5.944533824920654,
          7.095459938049316,
          5.95720100402832,
          6.005046367645264,
          6.010849475860596,
          5.472845077514648,
          4.83914852142334,
          6.031346797943115,
          5.26630163192749,
          6.156237602233887,
          6.0219855308532715,
          5.533731460571289,
          5.844489097595215,
          6.374021530151367,
          5.835978984832764,
          5.069180011749268,
          6.561822414398193,
          6.5075154304504395,
          5.619697570800781,
          6.472039699554443,
          6.587292194366455,
          6.338100910186768,
          4.982403755187988,
          6.329806327819824,
          6.183834075927734,
          6.264448642730713,
          6.050034999847412,
          5.415013790130615,
          5.516069412231445,
          6.669349670410156,
          6.144947528839111,
          5.393394947052002,
          6.215752601623535,
          5.57174825668335,
          4.818094730377197,
          6.687744617462158,
          5.411219120025635,
          5.118525981903076,
          5.3265252113342285,
          6.460451126098633,
          6.183831214904785,
          4.836877822875977,
          6.574106216430664,
          6.470272541046143,
          5.947898864746094,
          5.402161121368408,
          6.442848205566406,
          6.281447887420654,
          6.045928478240967,
          6.235348701477051,
          6.234813213348389,
          6.0059709548950195,
          5.489678382873535,
          5.525477409362793,
          6.145823001861572,
          6.093942642211914,
          6.357426643371582,
          5.092621803283691,
          5.989291191101074,
          6.370736122131348,
          5.9946608543396,
          6.4815754890441895,
          5.756338596343994,
          6.8081254959106445,
          5.4328718185424805,
          5.15868616104126,
          6.518832683563232,
          6.186371326446533,
          6.157346725463867,
          6.831949234008789,
          6.2747368812561035,
          5.678422927856445,
          6.315364837646484,
          5.055294990539551,
          6.258962631225586,
          6.2017741203308105,
          6.530533313751221,
          5.9291791915893555,
          6.31853723526001,
          6.0367960929870605,
          5.608599662780762,
          4.6980509757995605,
          5.985500335693359,
          5.383297443389893,
          5.127241134643555,
          6.264425277709961,
          6.426089763641357,
          5.884771347045898,
          4.9498724937438965,
          4.765376567840576,
          6.22321081161499,
          5.892844200134277,
          6.72017240524292,
          5.908953666687012,
          5.337568759918213,
          4.862103462219238,
          6.001543998718262,
          5.850539684295654,
          5.317132949829102,
          6.147188186645508,
          6.602303504943848,
          6.257719993591309,
          6.63765287399292,
          4.457849979400635,
          6.318670749664307,
          6.550384044647217,
          6.371571063995361,
          7.02487325668335,
          6.228944778442383,
          5.611169815063477,
          5.782009124755859,
          6.086620807647705,
          5.119751930236816,
          6.1139397621154785,
          5.997594356536865,
          5.991697311401367,
          6.006904602050781,
          6.377539157867432,
          6.923648834228516,
          6.622042655944824,
          5.921405792236328,
          6.662960529327393,
          6.6087470054626465,
          6.07175350189209,
          5.652802467346191,
          5.915195941925049,
          5.643461227416992,
          5.675160884857178,
          5.882086753845215,
          5.615476608276367,
          6.15078592300415,
          5.624383926391602,
          5.9593024253845215,
          6.347887992858887,
          6.140810966491699,
          6.295069694519043,
          6.344379425048828,
          5.859457015991211,
          6.028261184692383,
          5.981761455535889,
          5.609710216522217,
          5.0471720695495605,
          5.687454700469971,
          5.689119815826416,
          5.4923882484436035,
          6.131054878234863,
          5.222419738769531,
          5.774319648742676,
          5.789612293243408,
          6.662824630737305,
          4.254339218139648,
          6.155185699462891,
          4.7152099609375,
          6.6265339851379395,
          5.990035057067871,
          4.983309745788574,
          5.9262518882751465,
          6.527205944061279,
          6.461983680725098,
          7.006797790527344,
          5.336240768432617,
          5.635094165802002,
          6.420559406280518,
          5.9885454177856445,
          6.185024738311768,
          6.862427711486816,
          6.685133457183838,
          6.438499927520752,
          5.434272289276123,
          6.660399436950684,
          6.04121208190918,
          5.477505207061768,
          6.018113136291504,
          6.483262538909912,
          5.2144246101379395,
          6.376533031463623,
          6.449619770050049,
          5.756754398345947,
          6.574096202850342,
          6.468413352966309,
          5.965920925140381,
          5.015377998352051,
          5.307749271392822,
          5.280113220214844,
          5.777003765106201,
          5.465625762939453,
          6.697856426239014,
          6.232030391693115,
          5.5792412757873535,
          5.5546746253967285,
          6.232265472412109,
          6.6947503089904785,
          5.425622463226318,
          5.692624092102051,
          6.2952961921691895,
          5.286664962768555,
          5.440356254577637,
          5.000317096710205,
          5.510620594024658,
          4.722835540771484,
          5.65557336807251,
          6.082472801208496,
          6.139605522155762,
          4.878320693969727,
          5.675544261932373,
          4.864604949951172,
          5.872333526611328,
          6.082032680511475,
          5.787966251373291,
          4.5652971267700195,
          5.969338893890381,
          5.825263977050781,
          6.075545310974121,
          5.0827484130859375,
          6.341567039489746,
          6.098274230957031,
          6.12856912612915,
          5.61318826675415,
          5.732974052429199,
          6.416505813598633,
          5.173254013061523,
          5.072850704193115,
          6.3861799240112305,
          5.234088897705078,
          6.142953872680664,
          6.248007774353027,
          6.35767936706543,
          4.175353050231934,
          5.315375328063965,
          5.7867207527160645,
          5.98675012588501,
          6.01257848739624,
          6.1722331047058105,
          6.163760185241699,
          6.537969589233398,
          5.6972150802612305,
          5.822424411773682,
          5.518707275390625,
          6.126394748687744,
          6.613148212432861,
          5.905606746673584,
          5.457696437835693,
          4.949853897094727,
          6.497236728668213,
          6.362333297729492,
          5.985326766967773,
          6.217569828033447,
          6.4061079025268555,
          5.1214599609375,
          6.512955188751221,
          5.916839122772217,
          5.995694160461426,
          5.670201778411865,
          5.842394828796387,
          6.167954444885254,
          6.349600791931152,
          5.857113361358643,
          5.399579048156738,
          5.389247417449951,
          6.1767473220825195,
          6.480260848999023,
          5.763548851013184,
          6.095729351043701,
          4.516096591949463,
          5.120087623596191,
          6.657573699951172,
          6.443026542663574,
          6.304327011108398,
          5.615268707275391,
          5.437095642089844,
          6.515746593475342,
          5.468420505523682,
          6.489583969116211,
          4.992473125457764,
          5.427477836608887,
          6.270936012268066,
          6.2068023681640625,
          6.014164447784424,
          5.778295040130615,
          6.12241792678833,
          5.28153657913208,
          5.101290702819824,
          5.224151611328125,
          6.608959674835205,
          4.857878684997559,
          5.502399444580078,
          6.255655288696289,
          6.517165660858154,
          5.941339015960693,
          5.620988845825195,
          6.106900691986084,
          4.982522964477539,
          5.860632419586182,
          6.253450393676758,
          5.675398349761963,
          4.6239728927612305,
          6.060319900512695,
          6.08093786239624,
          4.832785606384277,
          6.541242599487305,
          6.035219192504883,
          4.477522850036621,
          6.3931379318237305,
          6.04879903793335,
          5.5138349533081055,
          6.288059234619141,
          5.939549922943115,
          5.930992603302002,
          5.5189714431762695,
          5.088662147521973,
          5.694723129272461,
          6.775018215179443,
          6.100700855255127,
          5.05099630355835,
          6.396470069885254,
          6.217515468597412,
          5.161748886108398,
          5.605055809020996,
          6.560050010681152,
          5.398238182067871,
          6.094011306762695,
          4.665069580078125,
          6.274457931518555,
          6.542900562286377,
          6.33557653427124,
          6.630542755126953,
          4.763823986053467,
          6.187264442443848,
          6.528395175933838,
          4.904305458068848,
          4.887606143951416,
          5.659982681274414,
          5.257596969604492,
          6.33456563949585,
          5.333182334899902,
          6.32205867767334,
          5.016100883483887,
          6.181178092956543,
          5.52330207824707,
          4.8730926513671875,
          5.029932975769043,
          6.082516193389893,
          6.5531744956970215
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Training curve for my tiny demo model!"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Tokens"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.0.min.js\"></script>                <div id=\"34d0382c-cf17-46fc-a981-a79cbd80db0e\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"34d0382c-cf17-46fc-a981-a79cbd80db0e\")) {                    Plotly.newPlot(                        \"34d0382c-cf17-46fc-a981-a79cbd80db0e\",                        [{\"hovertemplate\":\"Tokens=%{x}\\u003cbr\\u003eLoss=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"showlegend\":false,\"x\":[0,2048,4096,6144,8192,10240,12288,14336,16384,18432,20480,22528,24576,26624,28672,30720,32768,34816,36864,38912,40960,43008,45056,47104,49152,51200,53248,55296,57344,59392,61440,63488,65536,67584,69632,71680,73728,75776,77824,79872,81920,83968,86016,88064,90112,92160,94208,96256,98304,100352,102400,104448,106496,108544,110592,112640,114688,116736,118784,120832,122880,124928,126976,129024,131072,133120,135168,137216,139264,141312,143360,145408,147456,149504,151552,153600,155648,157696,159744,161792,163840,165888,167936,169984,172032,174080,176128,178176,180224,182272,184320,186368,188416,190464,192512,194560,196608,198656,200704,202752,204800,206848,208896,210944,212992,215040,217088,219136,221184,223232,225280,227328,229376,231424,233472,235520,237568,239616,241664,243712,245760,247808,249856,251904,253952,256000,258048,260096,262144,264192,266240,268288,270336,272384,274432,276480,278528,280576,282624,284672,286720,288768,290816,292864,294912,296960,299008,301056,303104,305152,307200,309248,311296,313344,315392,317440,319488,321536,323584,325632,327680,329728,331776,333824,335872,337920,339968,342016,344064,346112,348160,350208,352256,354304,356352,358400,360448,362496,364544,366592,368640,370688,372736,374784,376832,378880,380928,382976,385024,387072,389120,391168,393216,395264,397312,399360,401408,403456,405504,407552,409600,411648,413696,415744,417792,419840,421888,423936,425984,428032,430080,432128,434176,436224,438272,440320,442368,444416,446464,448512,450560,452608,454656,456704,458752,460800,462848,464896,466944,468992,471040,473088,475136,477184,479232,481280,483328,485376,487424,489472,491520,493568,495616,497664,499712,501760,503808,505856,507904,509952,512000,514048,516096,518144,520192,522240,524288,526336,528384,530432,532480,534528,536576,538624,540672,542720,544768,546816,548864,550912,552960,555008,557056,559104,561152,563200,565248,567296,569344,571392,573440,575488,577536,579584,581632,583680,585728,587776,589824,591872,593920,595968,598016,600064,602112,604160,606208,608256,610304,612352,614400,616448,618496,620544,622592,624640,626688,628736,630784,632832,634880,636928,638976,641024,643072,645120,647168,649216,651264,653312,655360,657408,659456,661504,663552,665600,667648,669696,671744,673792,675840,677888,679936,681984,684032,686080,688128,690176,692224,694272,696320,698368,700416,702464,704512,706560,708608,710656,712704,714752,716800,718848,720896,722944,724992,727040,729088,731136,733184,735232,737280,739328,741376,743424,745472,747520,749568,751616,753664,755712,757760,759808,761856,763904,765952,768000,770048,772096,774144,776192,778240,780288,782336,784384,786432,788480,790528,792576,794624,796672,798720,800768,802816,804864,806912,808960,811008,813056,815104,817152,819200,821248,823296,825344,827392,829440,831488,833536,835584,837632,839680,841728,843776,845824,847872,849920,851968,854016,856064,858112,860160,862208,864256,866304,868352,870400,872448,874496,876544,878592,880640,882688,884736,886784,888832,890880,892928,894976,897024,899072,901120,903168,905216,907264,909312,911360,913408,915456,917504,919552,921600,923648,925696,927744,929792,931840,933888,935936,937984,940032,942080,944128,946176,948224,950272,952320,954368,956416,958464,960512,962560,964608,966656,968704,970752,972800,974848,976896,978944,980992,983040,985088,987136,989184,991232,993280,995328,997376,999424,1001472,1003520,1005568,1007616,1009664,1011712,1013760,1015808,1017856,1019904,1021952,1024000,1026048,1028096,1030144,1032192,1034240,1036288,1038336,1040384,1042432,1044480,1046528,1048576,1050624,1052672,1054720,1056768,1058816,1060864,1062912,1064960,1067008,1069056,1071104,1073152,1075200,1077248,1079296,1081344,1083392,1085440,1087488,1089536,1091584,1093632,1095680,1097728,1099776,1101824,1103872,1105920,1107968,1110016,1112064,1114112,1116160,1118208,1120256,1122304,1124352,1126400,1128448,1130496,1132544,1134592,1136640,1138688,1140736,1142784,1144832,1146880,1148928,1150976,1153024,1155072,1157120,1159168,1161216,1163264,1165312,1167360,1169408,1171456,1173504,1175552,1177600,1179648,1181696,1183744,1185792,1187840,1189888,1191936,1193984,1196032,1198080,1200128,1202176,1204224,1206272,1208320,1210368,1212416,1214464,1216512,1218560,1220608,1222656,1224704,1226752,1228800,1230848,1232896,1234944,1236992,1239040,1241088,1243136,1245184,1247232,1249280,1251328,1253376,1255424,1257472,1259520,1261568,1263616,1265664,1267712,1269760,1271808,1273856,1275904,1277952,1280000,1282048,1284096,1286144,1288192,1290240,1292288,1294336,1296384,1298432,1300480,1302528,1304576,1306624,1308672,1310720,1312768,1314816,1316864,1318912,1320960,1323008,1325056,1327104,1329152,1331200,1333248,1335296,1337344,1339392,1341440,1343488,1345536,1347584,1349632,1351680,1353728,1355776,1357824,1359872,1361920,1363968,1366016,1368064,1370112,1372160,1374208,1376256,1378304,1380352,1382400,1384448,1386496,1388544,1390592,1392640,1394688,1396736,1398784,1400832,1402880,1404928,1406976,1409024,1411072,1413120,1415168,1417216,1419264,1421312,1423360,1425408,1427456,1429504,1431552,1433600,1435648,1437696,1439744,1441792,1443840,1445888,1447936,1449984,1452032,1454080,1456128,1458176,1460224,1462272,1464320,1466368,1468416,1470464,1472512,1474560,1476608,1478656,1480704,1482752,1484800,1486848,1488896,1490944,1492992,1495040,1497088,1499136,1501184,1503232,1505280,1507328,1509376,1511424,1513472,1515520,1517568,1519616,1521664,1523712,1525760,1527808,1529856,1531904,1533952,1536000,1538048,1540096,1542144,1544192,1546240,1548288,1550336,1552384,1554432,1556480,1558528,1560576,1562624,1564672,1566720,1568768,1570816,1572864,1574912,1576960,1579008,1581056,1583104,1585152,1587200,1589248,1591296,1593344,1595392,1597440,1599488,1601536,1603584,1605632,1607680,1609728,1611776,1613824,1615872,1617920,1619968,1622016,1624064,1626112,1628160,1630208,1632256,1634304,1636352,1638400,1640448,1642496,1644544,1646592,1648640,1650688,1652736,1654784,1656832,1658880,1660928,1662976,1665024,1667072,1669120,1671168,1673216,1675264,1677312,1679360,1681408,1683456,1685504,1687552,1689600,1691648,1693696,1695744,1697792,1699840,1701888,1703936,1705984,1708032,1710080,1712128,1714176,1716224,1718272,1720320,1722368,1724416,1726464,1728512,1730560,1732608,1734656,1736704,1738752,1740800,1742848,1744896,1746944,1748992,1751040,1753088,1755136,1757184,1759232,1761280,1763328,1765376,1767424,1769472,1771520,1773568,1775616,1777664,1779712,1781760,1783808,1785856,1787904,1789952,1792000,1794048,1796096,1798144,1800192,1802240,1804288,1806336,1808384,1810432,1812480,1814528,1816576,1818624,1820672,1822720,1824768,1826816,1828864,1830912,1832960,1835008,1837056,1839104,1841152,1843200,1845248,1847296,1849344,1851392,1853440,1855488,1857536,1859584,1861632,1863680,1865728,1867776,1869824,1871872,1873920,1875968,1878016,1880064,1882112,1884160,1886208,1888256,1890304,1892352,1894400,1896448,1898496,1900544,1902592,1904640,1906688,1908736,1910784,1912832,1914880,1916928,1918976,1921024,1923072,1925120,1927168,1929216,1931264,1933312,1935360,1937408,1939456,1941504,1943552,1945600,1947648,1949696,1951744,1953792,1955840,1957888,1959936,1961984,1964032,1966080,1968128,1970176,1972224,1974272,1976320,1978368,1980416,1982464,1984512,1986560,1988608,1990656,1992704,1994752,1996800,1998848,2000896,2002944,2004992,2007040,2009088,2011136,2013184,2015232,2017280,2019328,2021376,2023424,2025472,2027520,2029568,2031616,2033664,2035712,2037760,2039808,2041856,2043904,2045952,2048000,2050048],\"xaxis\":\"x\",\"y\":[10.854450225830078,10.463028907775879,10.383688926696777,9.776139259338379,9.935223579406738,9.540785789489746,9.517547607421875,8.807174682617188,8.66366958618164,8.67220401763916,8.646254539489746,8.42113971710205,7.951788425445557,8.28551197052002,7.5558857917785645,8.289531707763672,7.331459999084473,7.155359268188477,6.948245048522949,7.259580612182617,7.860927104949951,8.002614974975586,8.191458702087402,8.62995719909668,6.445733070373535,6.918249607086182,5.920351028442383,7.665004253387451,8.17246150970459,6.877509117126465,7.481996059417725,8.20982551574707,8.243618965148926,7.20925235748291,7.682490825653076,7.564950942993164,7.827813625335693,7.9076457023620605,5.502355575561523,8.095667839050293,6.812557220458984,7.712421417236328,6.73745584487915,7.168408393859863,8.01118278503418,8.143072128295898,7.743032455444336,6.729619026184082,6.296523571014404,6.803936958312988,7.869935035705566,7.378191947937012,7.625899314880371,8.260580062866211,7.043929576873779,7.457998752593994,7.156436443328857,7.0353546142578125,6.438826084136963,7.5832200050354,6.797715663909912,6.61657190322876,6.887738227844238,7.793162822723389,7.819830417633057,5.485851287841797,7.441709041595459,6.917121887207031,5.862812042236328,6.9526190757751465,7.7625603675842285,7.358996868133545,7.848251819610596,7.678569316864014,7.508083343505859,6.041210174560547,7.231173515319824,7.069821357727051,7.280869960784912,7.8199944496154785,7.134366512298584,6.509840488433838,7.917928695678711,7.603498458862305,7.646491050720215,7.432402610778809,7.537469863891602,6.594364643096924,6.112136363983154,7.6856160163879395,6.009410381317139,6.745038032531738,7.4962592124938965,7.712955951690674,7.570219993591309,6.636895656585693,7.595190048217773,6.400562286376953,7.5872416496276855,7.519845962524414,7.183290958404541,7.42397403717041,7.816910266876221,7.317833423614502,7.682782173156738,7.112697601318359,7.511951923370361,7.661698818206787,6.5653862953186035,7.605839252471924,7.622527122497559,6.800365447998047,7.028132438659668,6.904405117034912,6.619532108306885,7.835614204406738,7.360959053039551,6.969531059265137,7.386767864227295,5.361082077026367,7.026078224182129,6.2272562980651855,7.423372268676758,7.476095199584961,6.431253433227539,7.564887046813965,7.296453952789307,6.215089797973633,7.22170352935791,6.807578086853027,6.208980560302734,6.2713704109191895,7.362666130065918,7.297800064086914,6.010257244110107,7.495741367340088,7.115799427032471,6.621230125427246,7.421770095825195,6.946279048919678,6.279150009155273,6.676866054534912,7.226798057556152,6.858416557312012,7.5043158531188965,6.529477596282959,7.343371868133545,7.496452808380127,6.365985870361328,6.2831034660339355,7.350709438323975,7.375933647155762,6.78054666519165,6.12939977645874,7.844069957733154,6.7953104972839355,6.250616550445557,6.462862491607666,6.696166038513184,6.892684459686279,5.885461807250977,6.652431488037109,7.249911308288574,7.267889499664307,7.115579605102539,7.090950965881348,7.832364082336426,6.951417922973633,7.528704643249512,7.597931861877441,7.461143970489502,7.205909252166748,6.815666198730469,6.47885799407959,7.524909019470215,6.258906841278076,6.895514488220215,6.964444160461426,6.179443359375,7.5113420486450195,6.662262439727783,6.91325044631958,6.452484607696533,5.9713969230651855,7.58229923248291,7.09904670715332,6.2139387130737305,7.296949863433838,6.415127754211426,7.032045364379883,5.629449844360352,6.864357948303223,7.072058200836182,7.470970153808594,6.9340996742248535,6.087735652923584,7.016051292419434,6.916079044342041,7.58107328414917,7.1422438621521,7.183753490447998,6.901201248168945,7.138371467590332,6.669116973876953,6.936522483825684,5.085166931152344,4.692669868469238,6.803583145141602,6.952923774719238,6.505000591278076,6.466141223907471,7.350580215454102,7.41052770614624,6.603331089019775,6.825038433074951,6.291366100311279,7.19889497756958,6.922217845916748,6.664485931396484,7.409808158874512,7.514760494232178,7.234734535217285,7.350885391235352,6.564659118652344,7.0379862785339355,6.2766547203063965,6.621244430541992,6.42045783996582,6.09067964553833,5.8518571853637695,7.218234062194824,5.062824726104736,6.467580318450928,7.173182964324951,7.05493688583374,6.943305492401123,7.260612487792969,7.41037654876709,6.722615718841553,7.32243013381958,6.278411865234375,6.666016578674316,6.746776103973389,6.519835472106934,5.940471172332764,6.506335258483887,6.1811113357543945,6.053864002227783,7.578948020935059,7.1554274559021,6.4559502601623535,7.057476997375488,6.821982383728027,6.645841121673584,6.899338722229004,7.183646202087402,7.183658599853516,6.009227275848389,6.7261643409729,6.572842597961426,6.695363998413086,6.398828506469727,6.645105838775635,6.391038417816162,6.849864482879639,6.943141460418701,5.635378360748291,6.468249320983887,5.875392436981201,6.488244533538818,6.66205358505249,7.511533260345459,5.729803562164307,5.562570571899414,7.2957763671875,7.138801097869873,6.775303840637207,7.396947383880615,6.4913811683654785,6.554899215698242,6.784169673919678,7.1439032554626465,6.045857906341553,6.638937950134277,7.2307000160217285,5.966411113739014,6.997823715209961,7.074563026428223,6.1482253074646,6.262328624725342,6.073430061340332,6.794606685638428,6.065927505493164,4.977292060852051,6.78422737121582,7.097419261932373,6.059145450592041,4.185710906982422,6.566164970397949,7.128326416015625,6.887077331542969,6.601691246032715,6.861466407775879,6.752424716949463,7.457740306854248,5.896881580352783,5.89732551574707,6.059147834777832,6.363085746765137,6.01101541519165,6.510138511657715,6.312315940856934,7.058523178100586,6.380595684051514,6.561495780944824,6.211428165435791,6.913214683532715,6.152157783508301,6.933550834655762,7.3156232833862305,6.40919828414917,6.5238518714904785,6.801835536956787,7.238858699798584,6.506946086883545,7.298291206359863,6.940350532531738,4.696794509887695,6.194520950317383,6.555710792541504,6.688409805297852,6.030770301818848,6.032138824462891,6.5558061599731445,5.7547197341918945,4.9841694831848145,6.733280181884766,7.1334452629089355,7.132935523986816,5.98329496383667,6.251224040985107,6.1005401611328125,5.606653213500977,5.370863914489746,7.127809524536133,5.751622676849365,7.168690204620361,7.13054895401001,6.764644145965576,6.809328556060791,5.847984790802002,6.864226818084717,6.547879219055176,6.492173194885254,6.84027624130249,5.860219955444336,4.866832733154297,6.342408657073975,5.843356609344482,6.596684455871582,7.435032844543457,6.478963851928711,6.323365688323975,6.918969631195068,6.3177995681762695,6.713742733001709,6.843751430511475,6.512274742126465,6.446118354797363,6.9718098640441895,7.284698486328125,5.909122943878174,6.8837175369262695,6.591510772705078,6.729146957397461,6.964982986450195,6.496385097503662,7.100907802581787,6.68963098526001,5.974977493286133,5.922025680541992,6.133410453796387,6.02763032913208,6.242808818817139,6.801566123962402,6.399755001068115,6.366701602935791,6.995099067687988,5.918675422668457,6.6862640380859375,6.45829439163208,5.880483627319336,6.6393818855285645,5.778737545013428,6.828270435333252,6.008747577667236,5.958245277404785,6.924577713012695,6.0521626472473145,7.347006320953369,6.226801872253418,6.219110012054443,6.761191368103027,6.599098205566406,6.150038719177246,6.075898170471191,6.9524760246276855,5.565514087677002,5.938342571258545,6.821135997772217,7.076663017272949,5.66713285446167,6.317336082458496,5.914614677429199,7.116015434265137,6.374999046325684,5.8701863288879395,7.394593238830566,5.239419460296631,6.751798629760742,6.524456024169922,6.689646244049072,6.317525863647461,7.033802509307861,6.637807369232178,6.574493885040283,7.198011875152588,6.692473888397217,6.262240886688232,5.771246433258057,6.527289390563965,6.774779319763184,6.538018703460693,6.132085800170898,5.728128910064697,6.366733551025391,5.733404159545898,6.929437637329102,5.862786769866943,4.678160190582275,5.961729049682617,5.501026153564453,5.150195121765137,6.614116668701172,5.832125186920166,5.766794204711914,5.465226650238037,6.625421047210693,6.925655364990234,5.964712619781494,6.664275169372559,5.63389778137207,5.943923473358154,7.198044776916504,6.581083297729492,6.3163981437683105,5.9027910232543945,6.156276226043701,5.892621040344238,6.4538068771362305,5.536754131317139,4.611171245574951,5.5808916091918945,6.369757175445557,5.02683162689209,6.983375072479248,6.369698524475098,6.717695236206055,6.445165157318115,6.606196403503418,5.966333389282227,6.132497310638428,5.9362101554870605,6.412680625915527,6.387152671813965,6.509056091308594,6.136280059814453,6.5208420753479,7.014150142669678,7.141970634460449,5.999146938323975,6.3928656578063965,6.779301643371582,5.963932037353516,6.876800060272217,6.44705057144165,6.328127861022949,6.3202691078186035,5.52315616607666,5.674781322479248,6.519289970397949,5.665248870849609,6.407644748687744,6.916439056396484,6.315254211425781,5.883701801300049,6.401333808898926,5.865872383117676,6.398603916168213,6.40331506729126,6.908053874969482,6.5544586181640625,6.333073616027832,6.23280668258667,6.893930912017822,6.121990203857422,6.394949436187744,6.506539821624756,5.809802055358887,7.005299091339111,5.540140628814697,6.208792209625244,6.254007816314697,6.500553131103516,5.597997188568115,6.820410251617432,5.953258514404297,5.752680778503418,5.9722394943237305,6.3845295906066895,6.835316181182861,6.545851707458496,5.7922234535217285,5.978375434875488,4.608325958251953,6.76346492767334,5.7530198097229,6.517764091491699,4.347540378570557,6.52103328704834,6.421048641204834,6.133098125457764,6.979004383087158,5.200563907623291,6.642823219299316,6.166574001312256,6.542239189147949,5.717117786407471,6.0630717277526855,6.383273124694824,6.416958332061768,6.679996013641357,6.192503929138184,6.440696716308594,6.4084320068359375,6.63875675201416,6.894071102142334,6.484639644622803,5.7469916343688965,4.701140880584717,6.3789381980896,6.194981098175049,6.588928699493408,6.027205944061279,5.9799346923828125,5.648458480834961,5.312124729156494,6.874805450439453,6.776154518127441,5.820784568786621,6.691176891326904,7.0295000076293945,5.611313343048096,5.978029251098633,6.88893985748291,6.821957111358643,5.626959323883057,5.345090389251709,5.454472541809082,5.980249881744385,6.151882648468018,6.171768665313721,7.049464225769043,6.269687175750732,6.841146469116211,6.155098915100098,6.370288848876953,6.1082305908203125,6.549829006195068,5.592128276824951,4.703105926513672,5.816777229309082,5.977025985717773,6.0030412673950195,5.4569091796875,5.84878396987915,5.921636581420898,6.190796375274658,6.701752662658691,6.464745044708252,5.8600754737854,6.319988250732422,5.792882919311523,5.924829483032227,6.348631858825684,5.987783908843994,5.815739154815674,5.870761394500732,7.056538105010986,6.126522064208984,5.90033483505249,6.00835657119751,6.297720909118652,5.361133575439453,5.149258613586426,6.567466735839844,6.071683406829834,5.817484378814697,6.720035076141357,5.414102554321289,4.883605003356934,6.064277172088623,6.766464710235596,5.311984062194824,6.905226230621338,6.266335487365723,5.266031265258789,6.861542701721191,6.88754415512085,5.31223726272583,6.748654842376709,6.801799297332764,6.83816385269165,6.678792476654053,5.829503059387207,5.7914605140686035,6.736681938171387,6.98419713973999,6.938019275665283,5.815917491912842,6.641533374786377,6.316003799438477,6.168960094451904,5.220558166503906,6.5882039070129395,6.5039963722229,5.6971940994262695,6.511314868927002,6.278578758239746,6.029819488525391,5.032116889953613,6.294021129608154,6.550290584564209,7.071104049682617,6.35819673538208,6.800181865692139,5.4153032302856445,6.54008674621582,6.549932956695557,5.829015254974365,6.199897766113281,6.227444648742676,6.703969478607178,6.208791732788086,6.214728355407715,6.553897857666016,6.042068004608154,5.225064277648926,5.877416133880615,5.944533824920654,7.095459938049316,5.95720100402832,6.005046367645264,6.010849475860596,5.472845077514648,4.83914852142334,6.031346797943115,5.26630163192749,6.156237602233887,6.0219855308532715,5.533731460571289,5.844489097595215,6.374021530151367,5.835978984832764,5.069180011749268,6.561822414398193,6.5075154304504395,5.619697570800781,6.472039699554443,6.587292194366455,6.338100910186768,4.982403755187988,6.329806327819824,6.183834075927734,6.264448642730713,6.050034999847412,5.415013790130615,5.516069412231445,6.669349670410156,6.144947528839111,5.393394947052002,6.215752601623535,5.57174825668335,4.818094730377197,6.687744617462158,5.411219120025635,5.118525981903076,5.3265252113342285,6.460451126098633,6.183831214904785,4.836877822875977,6.574106216430664,6.470272541046143,5.947898864746094,5.402161121368408,6.442848205566406,6.281447887420654,6.045928478240967,6.235348701477051,6.234813213348389,6.0059709548950195,5.489678382873535,5.525477409362793,6.145823001861572,6.093942642211914,6.357426643371582,5.092621803283691,5.989291191101074,6.370736122131348,5.9946608543396,6.4815754890441895,5.756338596343994,6.8081254959106445,5.4328718185424805,5.15868616104126,6.518832683563232,6.186371326446533,6.157346725463867,6.831949234008789,6.2747368812561035,5.678422927856445,6.315364837646484,5.055294990539551,6.258962631225586,6.2017741203308105,6.530533313751221,5.9291791915893555,6.31853723526001,6.0367960929870605,5.608599662780762,4.6980509757995605,5.985500335693359,5.383297443389893,5.127241134643555,6.264425277709961,6.426089763641357,5.884771347045898,4.9498724937438965,4.765376567840576,6.22321081161499,5.892844200134277,6.72017240524292,5.908953666687012,5.337568759918213,4.862103462219238,6.001543998718262,5.850539684295654,5.317132949829102,6.147188186645508,6.602303504943848,6.257719993591309,6.63765287399292,4.457849979400635,6.318670749664307,6.550384044647217,6.371571063995361,7.02487325668335,6.228944778442383,5.611169815063477,5.782009124755859,6.086620807647705,5.119751930236816,6.1139397621154785,5.997594356536865,5.991697311401367,6.006904602050781,6.377539157867432,6.923648834228516,6.622042655944824,5.921405792236328,6.662960529327393,6.6087470054626465,6.07175350189209,5.652802467346191,5.915195941925049,5.643461227416992,5.675160884857178,5.882086753845215,5.615476608276367,6.15078592300415,5.624383926391602,5.9593024253845215,6.347887992858887,6.140810966491699,6.295069694519043,6.344379425048828,5.859457015991211,6.028261184692383,5.981761455535889,5.609710216522217,5.0471720695495605,5.687454700469971,5.689119815826416,5.4923882484436035,6.131054878234863,5.222419738769531,5.774319648742676,5.789612293243408,6.662824630737305,4.254339218139648,6.155185699462891,4.7152099609375,6.6265339851379395,5.990035057067871,4.983309745788574,5.9262518882751465,6.527205944061279,6.461983680725098,7.006797790527344,5.336240768432617,5.635094165802002,6.420559406280518,5.9885454177856445,6.185024738311768,6.862427711486816,6.685133457183838,6.438499927520752,5.434272289276123,6.660399436950684,6.04121208190918,5.477505207061768,6.018113136291504,6.483262538909912,5.2144246101379395,6.376533031463623,6.449619770050049,5.756754398345947,6.574096202850342,6.468413352966309,5.965920925140381,5.015377998352051,5.307749271392822,5.280113220214844,5.777003765106201,5.465625762939453,6.697856426239014,6.232030391693115,5.5792412757873535,5.5546746253967285,6.232265472412109,6.6947503089904785,5.425622463226318,5.692624092102051,6.2952961921691895,5.286664962768555,5.440356254577637,5.000317096710205,5.510620594024658,4.722835540771484,5.65557336807251,6.082472801208496,6.139605522155762,4.878320693969727,5.675544261932373,4.864604949951172,5.872333526611328,6.082032680511475,5.787966251373291,4.5652971267700195,5.969338893890381,5.825263977050781,6.075545310974121,5.0827484130859375,6.341567039489746,6.098274230957031,6.12856912612915,5.61318826675415,5.732974052429199,6.416505813598633,5.173254013061523,5.072850704193115,6.3861799240112305,5.234088897705078,6.142953872680664,6.248007774353027,6.35767936706543,4.175353050231934,5.315375328063965,5.7867207527160645,5.98675012588501,6.01257848739624,6.1722331047058105,6.163760185241699,6.537969589233398,5.6972150802612305,5.822424411773682,5.518707275390625,6.126394748687744,6.613148212432861,5.905606746673584,5.457696437835693,4.949853897094727,6.497236728668213,6.362333297729492,5.985326766967773,6.217569828033447,6.4061079025268555,5.1214599609375,6.512955188751221,5.916839122772217,5.995694160461426,5.670201778411865,5.842394828796387,6.167954444885254,6.349600791931152,5.857113361358643,5.399579048156738,5.389247417449951,6.1767473220825195,6.480260848999023,5.763548851013184,6.095729351043701,4.516096591949463,5.120087623596191,6.657573699951172,6.443026542663574,6.304327011108398,5.615268707275391,5.437095642089844,6.515746593475342,5.468420505523682,6.489583969116211,4.992473125457764,5.427477836608887,6.270936012268066,6.2068023681640625,6.014164447784424,5.778295040130615,6.12241792678833,5.28153657913208,5.101290702819824,5.224151611328125,6.608959674835205,4.857878684997559,5.502399444580078,6.255655288696289,6.517165660858154,5.941339015960693,5.620988845825195,6.106900691986084,4.982522964477539,5.860632419586182,6.253450393676758,5.675398349761963,4.6239728927612305,6.060319900512695,6.08093786239624,4.832785606384277,6.541242599487305,6.035219192504883,4.477522850036621,6.3931379318237305,6.04879903793335,5.5138349533081055,6.288059234619141,5.939549922943115,5.930992603302002,5.5189714431762695,5.088662147521973,5.694723129272461,6.775018215179443,6.100700855255127,5.05099630355835,6.396470069885254,6.217515468597412,5.161748886108398,5.605055809020996,6.560050010681152,5.398238182067871,6.094011306762695,4.665069580078125,6.274457931518555,6.542900562286377,6.33557653427124,6.630542755126953,4.763823986053467,6.187264442443848,6.528395175933838,4.904305458068848,4.887606143951416,5.659982681274414,5.257596969604492,6.33456563949585,5.333182334899902,6.32205867767334,5.016100883483887,6.181178092956543,5.52330207824707,4.8730926513671875,5.029932975769043,6.082516193389893,6.5531744956970215],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Loss\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Training curve for my tiny demo model!\"}},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'hovertemplate': 'Tokens=%{x}<br>Loss=%{y}<extra></extra>',\n",
       "              'legendgroup': '',\n",
       "              'line': {'color': '#636efa', 'dash': 'solid'},\n",
       "              'marker': {'symbol': 'circle'},\n",
       "              'mode': 'lines',\n",
       "              'name': '',\n",
       "              'showlegend': False,\n",
       "              'type': 'scattergl',\n",
       "              'x': array([      0,    2048,    4096, ..., 2045952, 2048000, 2050048]),\n",
       "              'xaxis': 'x',\n",
       "              'y': array([10.85445023, 10.46302891, 10.38368893, ...,  5.02993298,  6.08251619,\n",
       "                           6.5531745 ]),\n",
       "              'yaxis': 'y'}],\n",
       "    'layout': {'legend': {'tracegroupgap': 0},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Training curve for my tiny demo model!'},\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'Tokens'}},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.line(\n",
    "    y=losses,\n",
    "    x=np.arange(len(losses)) * (model_cfg.n_ctx * batch_size),\n",
    "    labels={\"y\": \"Loss\", \"x\": \"Tokens\"},\n",
    "    title=\"Training curve for my tiny demo model!\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
