{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [What is a Transformer?](https://youtu.be/bOYE6E8JrtU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "import tqdm.auto as tqdm\n",
    "# from unseal import transformers_util as tutil\n",
    "# from unseal import hooks\n",
    "# import unseal.visuals.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gge/gg2001/transformer-circuits/.conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    model_name, fold_ln=False, center_unembed=False, center_writing_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseal_gpt2, unseal_tokenizer, unseal_config = tutil.load_from_pretrained(model_name)\n",
    "# unseal_gpt2.to(device)\n",
    "# hooked_gpt2 = hooks.HookedModel(unseal_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256, 15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,\n",
      "          6067,     0]], device='mps:0')\n",
      "tensor([[15354,   257,  1573,  6140,   351,   257,  3139,   393,  2272,  6067,\n",
      "             0]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    reference_gpt2.to_tokens(\"Whether a word begins with a capital or space matters!\")\n",
    ")\n",
    "print(\n",
    "    reference_gpt2.to_tokens(\n",
    "        \"Whether a word begins with a capital or space matters!\", prepend_bos=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed.W_E', 'pos_embed.W_pos', 'blocks.0.ln1.w', 'blocks.0.ln1.b', 'blocks.0.ln2.w', 'blocks.0.ln2.b', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.mlp.W_in', 'blocks.0.mlp.b_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.b_out', 'blocks.1.ln1.w', 'blocks.1.ln1.b', 'blocks.1.ln2.w', 'blocks.1.ln2.b', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.mlp.W_in', 'blocks.1.mlp.b_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.b_out', 'blocks.2.ln1.w', 'blocks.2.ln1.b', 'blocks.2.ln2.w', 'blocks.2.ln2.b', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.mlp.W_in', 'blocks.2.mlp.b_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.b_out', 'blocks.3.ln1.w', 'blocks.3.ln1.b', 'blocks.3.ln2.w', 'blocks.3.ln2.b', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.mlp.W_in', 'blocks.3.mlp.b_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.b_out', 'blocks.4.ln1.w', 'blocks.4.ln1.b', 'blocks.4.ln2.w', 'blocks.4.ln2.b', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.mlp.W_in', 'blocks.4.mlp.b_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.b_out', 'blocks.5.ln1.w', 'blocks.5.ln1.b', 'blocks.5.ln2.w', 'blocks.5.ln2.b', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.mlp.W_in', 'blocks.5.mlp.b_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.b_out', 'blocks.6.ln1.w', 'blocks.6.ln1.b', 'blocks.6.ln2.w', 'blocks.6.ln2.b', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_O', 'blocks.6.attn.W_K', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_V', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.mlp.W_in', 'blocks.6.mlp.b_in', 'blocks.6.mlp.W_out', 'blocks.6.mlp.b_out', 'blocks.7.ln1.w', 'blocks.7.ln1.b', 'blocks.7.ln2.w', 'blocks.7.ln2.b', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_O', 'blocks.7.attn.W_K', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_V', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.mlp.W_in', 'blocks.7.mlp.b_in', 'blocks.7.mlp.W_out', 'blocks.7.mlp.b_out', 'blocks.8.ln1.w', 'blocks.8.ln1.b', 'blocks.8.ln2.w', 'blocks.8.ln2.b', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_O', 'blocks.8.attn.W_K', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_V', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.mlp.W_in', 'blocks.8.mlp.b_in', 'blocks.8.mlp.W_out', 'blocks.8.mlp.b_out', 'blocks.9.ln1.w', 'blocks.9.ln1.b', 'blocks.9.ln2.w', 'blocks.9.ln2.b', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_O', 'blocks.9.attn.W_K', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_V', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.mlp.W_in', 'blocks.9.mlp.b_in', 'blocks.9.mlp.W_out', 'blocks.9.mlp.b_out', 'blocks.10.ln1.w', 'blocks.10.ln1.b', 'blocks.10.ln2.w', 'blocks.10.ln2.b', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_O', 'blocks.10.attn.W_K', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_V', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.mlp.W_in', 'blocks.10.mlp.b_in', 'blocks.10.mlp.W_out', 'blocks.10.mlp.b_out', 'blocks.11.ln1.w', 'blocks.11.ln1.b', 'blocks.11.ln2.w', 'blocks.11.ln2.b', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_O', 'blocks.11.attn.W_K', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_V', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.mlp.W_in', 'blocks.11.mlp.b_in', 'blocks.11.mlp.W_out', 'blocks.11.mlp.b_out', 'ln_final.w', 'ln_final.b', 'unembed.W_U', 'unembed.b_U'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '568',\n",
       " '73',\n",
       " '+',\n",
       " '318',\n",
       " '46',\n",
       " '23',\n",
       " '=',\n",
       " '123',\n",
       " '45',\n",
       " '67',\n",
       " '89',\n",
       " '-',\n",
       " '1',\n",
       " '000000',\n",
       " '000']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='mps:0')\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(tokens)\n",
    "# batch, position\n",
    "print(tokens.shape)\n",
    "print(reference_gpt2.to_str_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "# batch, position, d_vocab\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "log_probs = logits.log_softmax(dim=-1)\n",
    "probs = logits.log_softmax(dim=-1)\n",
    "print(log_probs.shape)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    zip(\n",
    "        reference_gpt2.to_str_tokens(reference_text),\n",
    "        reference_gpt2.tokenizer.batch_decode(\n",
    "            logits.argmax(dim=-1)[0]\n",
    "        ),  # Sample largest logit\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(314, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Map distribution to a token\n",
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Input: tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0,   314]], device='mps:0')\n",
      "torch.Size([1, 36])\n",
      "New Input: <|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I\n",
      "torch.Size([1, 36, 50257])\n",
      "tensor(716, device='mps:0')\n",
      " am\n"
     ]
    }
   ],
   "source": [
    "next_tokens = torch.cat([tokens, next_token.clone().detach()[None, None]], dim=-1)\n",
    "new_logits = reference_gpt2(next_tokens)\n",
    "print(\"New Input:\", next_tokens)\n",
    "print(next_tokens.shape)\n",
    "print(\"New Input:\", reference_gpt2.tokenizer.decode(next_tokens[0]))\n",
    "\n",
    "print(new_logits.shape)\n",
    "print(new_logits[-1, -1].argmax(-1))\n",
    "\n",
    "print(reference_gpt2.tokenizer.decode(new_logits[-1, -1].argmax(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 35, 768])\n",
      "hook_pos_embed torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
      "ln_final.hook_scale torch.Size([1, 35, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(activation_name, activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768])\n",
      "pos_embed.W_pos torch.Size([1024, 768])\n",
      "blocks.0.ln1.w torch.Size([768])\n",
      "blocks.0.ln1.b torch.Size([768])\n",
      "blocks.0.ln2.w torch.Size([768])\n",
      "blocks.0.ln2.b torch.Size([768])\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
      "blocks.0.attn.b_Q torch.Size([12, 64])\n",
      "blocks.0.attn.b_O torch.Size([768])\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
      "blocks.0.attn.b_K torch.Size([12, 64])\n",
      "blocks.0.attn.b_V torch.Size([12, 64])\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
      "blocks.0.mlp.b_in torch.Size([3072])\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
      "blocks.0.mlp.b_out torch.Size([768])\n",
      "ln_final.w torch.Size([768])\n",
      "ln_final.b torch.Size([768])\n",
      "unembed.W_U torch.Size([768, 50257])\n",
      "unembed.b_U torch.Size([50257])\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='mps'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768  # Embedding dimensions\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257  # Token dictionary size\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024  # Context length\n",
    "    d_head: int = 64  # Dimensions of each attention head, d_model // n_heads\n",
    "    d_mlp: int = 3072  # MLP hidden layer dimensions, d_model * 4\n",
    "    n_heads: int = 12  # Attention heads per block\n",
    "    n_layers: int = 12  # Number of transformer blocks\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = torch.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_gpt2_test(\n",
    "    cls,\n",
    "    gpt2_layer,\n",
    "    input_name,\n",
    "    cache_dict=cache.cache_dict,\n",
    "    argcount=False,\n",
    "):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Modify this part to handle the different input requirements\n",
    "    if argcount:\n",
    "        # If the gpt2_layer expects separate inputs for query, key, and value\n",
    "        reference_output = gpt2_layer(reference_input, reference_input, reference_input)\n",
    "    else:\n",
    "        # If the gpt2_layer expects a single input\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: torch.Tensor):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(\n",
    "            residual, \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )  # (batch, position, 1)\n",
    "        variance = einops.reduce(\n",
    "            residual.pow(2), \"batch position d_model -> batch position 1\", \"mean\"\n",
    "        )\n",
    "\n",
    "        normalized = (residual - mean) / (\n",
    "            variance + self.cfg.layer_norm_eps\n",
    "        ).sqrt()  # (batch, position, d_model)\n",
    "        out = normalized * self.w + self.b  # (batch, position, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "99.99% of the values are correct\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_E[tokens]  # [batch, position, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens: [batch, position]\n",
    "        return self.W_pos[\n",
    "            torch.arange(tokens.shape[1], device=device)  # [0, 1, 2, ..., position-1]\n",
    "        ]  # (position, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "          2.8267e-02,  5.4490e-02],\n",
       "        [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "          1.0172e-02, -1.5573e-04],\n",
       "        [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "          1.9325e-02, -2.1424e-02],\n",
       "        ...,\n",
       "        [ 4.6277e-04,  2.3037e-02,  4.1227e-02,  ..., -1.9287e-03,\n",
       "         -2.3037e-03, -4.3189e-03],\n",
       "        [-2.7136e-03,  2.1724e-02,  3.9675e-02,  ...,  4.2048e-04,\n",
       "         -4.8160e-03, -9.2252e-04],\n",
       "        [ 6.6815e-03,  2.0595e-02,  3.6596e-02,  ..., -9.5090e-04,\n",
       "         -3.2512e-03, -9.6509e-04]], device='mps:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_layers = tutil.get_num_layers(hooked_gpt2, layer_key_prefix=\"transformer->h\")\n",
    "# html_storage = dict()\n",
    "# html_storage = utils.compute_attn_logits(\n",
    "#     hooked_gpt2,\n",
    "#     model_name,\n",
    "#     unseal_tokenizer,\n",
    "#     num_layers,\n",
    "#     reference_text,\n",
    "#     html_storage,\n",
    "#     layer_key_prefix=\"transformer->h\",\n",
    "#     out_proj_name=\"c_proj\",\n",
    "#     batch_size=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_pre: torch.Tensor):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        q = (\n",
    "            einsum(\n",
    "                \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_Q,\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_K,\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "\n",
    "        scores = einsum(\n",
    "            \"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\",\n",
    "            q,\n",
    "            k,\n",
    "        )\n",
    "        scores = scores / math.sqrt(self.cfg.d_head)\n",
    "        scores = self.apply_causal_mask(scores)\n",
    "\n",
    "        scores = scores.softmax(dim=-1)  # (batch, n_head, query_pos, key_pos)\n",
    "\n",
    "        v = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_resid_pre,\n",
    "                self.W_V,\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        heads_output = einsum(\n",
    "            \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\",\n",
    "            scores,\n",
    "            v,\n",
    "        )\n",
    "\n",
    "        attn_output = (\n",
    "            einsum(\n",
    "                \"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\",\n",
    "                heads_output,\n",
    "                self.W_O,\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores: torch.Tensor):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(\n",
    "            torch.ones(\n",
    "                attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device\n",
    "            ),\n",
    "            diagonal=1,\n",
    "        ).bool()\n",
    "        attn_scores.masked_fill_(mask, float(\"-inf\"))\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.9663e-01,  1.6985e-02,  3.4781e-02,  ...,  3.3119e-02,\n",
       "          -2.3129e-02,  1.8103e-01],\n",
       "         [ 1.3155e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1998e-03,\n",
       "           5.3075e-03,  1.3511e-01],\n",
       "         [ 8.9737e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
       "           2.7958e-03,  9.0785e-02],\n",
       "         ...,\n",
       "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
       "          -4.9522e-04, -8.6008e-03],\n",
       "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6689e-02,\n",
       "          -1.4368e-02,  3.3245e-02],\n",
       "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
       "          -8.9028e-03,  4.8796e-02]]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(\n",
    "    Attention,\n",
    "    reference_gpt2.blocks[0].attn,\n",
    "    cache[\"blocks.0.ln1.hook_normalized\"],\n",
    "    argcount=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "    def forward(self, normalized_resid_mid: torch.Tensor):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        hidden_layer = (\n",
    "            einsum(\n",
    "                \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        activation = gelu_new(hidden_layer)\n",
    "        output_layer = (\n",
    "            einsum(\n",
    "                \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "                activation,\n",
    "                self.W_out,\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4380,  0.3624,  0.5117,  ...,  1.7227,  1.5761,  0.0368],\n",
       "         [-1.0766, -0.0438,  0.3276,  ..., -0.5437,  0.4033,  0.3717],\n",
       "         [-1.2182, -1.5481, -0.9702,  ...,  1.0737,  0.7199,  0.5080],\n",
       "         ...,\n",
       "         [-0.4004,  0.8475,  0.2047,  ...,  0.3789,  0.0455, -0.4744],\n",
       "         [-0.0862,  0.7839,  0.9046,  ..., -0.2174, -0.5953,  0.8555],\n",
       "         [ 0.8448, -0.3743,  1.0397,  ...,  0.0296,  0.3405,  0.3585]]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre: torch.Tensor):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "\n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "96.70% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3911,  0.1542,  0.6004,  ...,  1.7199,  1.7365,  0.3928],\n",
       "         [-0.9035, -0.0360,  0.2350,  ..., -0.4147,  0.3562,  0.3932],\n",
       "         [-0.9644, -2.4814, -1.4994,  ...,  1.4044,  0.7617,  0.5915],\n",
       "         ...,\n",
       "         [-0.7420,  0.9250, -0.3219,  ...,  0.2920,  0.1097, -0.5344],\n",
       "         [-1.3221,  0.8959,  1.1793,  ..., -0.5544, -0.4071,  0.9254],\n",
       "         [ 1.1209, -0.8919,  1.3737,  ..., -0.1356,  0.3435,  0.4517]]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(self, normalized_resid_final: torch.Tensor):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        logits = (\n",
    "            einsum(\n",
    "                \"batch position d_model, d_model d_vocab -> batch position d_vocab\",\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4317,  -39.8364,  -43.0660,  ...,  -54.0877,  -54.3451,\n",
       "           -42.3644],\n",
       "         [-128.0392, -127.9936, -130.7010,  ..., -136.7121, -129.9261,\n",
       "          -129.3965],\n",
       "         [-119.8521, -121.0064, -123.8819,  ..., -128.5180, -126.6028,\n",
       "          -121.9060],\n",
       "         ...,\n",
       "         [-112.9815, -112.7749, -117.0633,  ..., -121.2914, -117.6574,\n",
       "          -114.5005],\n",
       "         [ -98.6725, -104.4889, -108.7362,  ..., -118.3553, -113.8767,\n",
       "          -106.3605],\n",
       "         [-126.8285, -128.9596, -128.3941,  ..., -140.1970, -138.5883,\n",
       "          -122.3697]]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        residual = embed + pos_embed\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        normalized_resid_final = self.ln_final(residual)\n",
    "        logits = self.unembed(normalized_resid_final)\n",
    "        # logits have shape [batch, position, logits]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ -43.4543,  -39.8558,  -43.0849,  ...,  -54.1044,  -54.3628,\n",
       "           -42.3845],\n",
       "         [-128.0192, -127.9733, -130.6802,  ..., -136.6926, -129.9065,\n",
       "          -129.3768],\n",
       "         [-119.8427, -120.9960, -123.8709,  ..., -128.5087, -126.5927,\n",
       "          -121.8961],\n",
       "         ...,\n",
       "         [-112.9791, -112.7721, -117.0613,  ..., -121.2890, -117.6549,\n",
       "          -114.4980],\n",
       "         [ -98.6525, -104.4703, -108.7175,  ..., -118.3360, -113.8571,\n",
       "          -106.3412],\n",
       "         [-126.8190, -128.9513, -128.3855,  ..., -140.1897, -138.5799,\n",
       "          -122.3602]]], device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
