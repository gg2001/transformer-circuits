{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercises](https://transformer-circuits.pub/2021/exercises/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the transformer architecture at a high level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe how an individual attention head works in detail, in terms of the matrices $W_Q$, $W_K$, $W_V$, and $W_out$. (The equations and code for an attention head are often written for all attention heads in a layer concatenated together at once. This implementation is more computationally efficient, but harder to reason about, so we'd like to describe a single attention head.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention heads move information from a subspace of the residual stream of one token to a different subspace in the residual stream of another. Which matrix controls the subspace that gets read, and which matrix controls the subspace written to? What does their product mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which tokens an attention head attends to is controlled by only two of the four matrices that define an attention head. Which two matrices are these?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention heads can be written in terms of two matrices instead of four, $W_Q^T \\cdot W_k$ and $W_{out} \\cdot W_v$. In the previous two questions, you gave interpretations to these matrices. Now write out an attention head with only reference to them.\n",
    "\n",
    "#### What is the rank of these matrices?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You'd like to understand whether an attention head is reading in the output of a previous attention head. What does $W_V^2 \\cdot W_{out}^1$â€‹ tell you about this? What do the singular values tell you?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Building a simple virtual attention head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Copying Text with an Induction Head (Pointer Arithmetic Version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Copying Text with an Induction Head (Previous Token K-Composition Version)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
